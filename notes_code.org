* Some general notes on Merlin
  :LOGBOOK:
  CLOCK: [2021-03-29 Mon 17:20]--[2021-03-29 Mon 17:46] =>  0:26
  :END:

** From call with Radu of 18/03/2021

      - To run the code: dataset -> file to

     bte, cte -> exact inference.

     bmw, jjlp -> approximate inference.

     task -> mar (marginal), pr (probability of evidence),

     positive -> if 0 probabilities.

     iterations -> controls approximation schemas

     init-factors -> initialization of parameters.

     lock-factors -> some of the parameters you keep them fixed.

     output format -> json,

     .uai -> format, txt format, to represent graphical model.

     preable -> first entry, number of variables. second row the
     number of conditional parameters for each variables  - third line
     number of factors. 4 and 5th line arguments of function.

     cancer.uai -> see syntax for bayesian network. line 5 - 9 (first
     argument number , last number the child, all of the other parents).


** General Code Structure
   :LOGBOOK:
   CLOCK: [2021-05-29 Sat 16:28]--[2021-05-29 Sat 16:53] =>  0:25
   CLOCK: [2021-05-29 Sat 16:03]--[2021-05-29 Sat 16:28] =>  0:25
   :END:

   #+begin_src plantuml :file ./images/strucutre.png
   @startuml

   /' Class and Variables Declaration '/
   class Merlin
   class EM
   circle main
   class graphical_model
   class cte
   class observation

   /'General Notes and Comments'/

       /'Graphical Model'/
       note top of graphical_model: this parses the uai file into a graph strucutre.

       /'Main'/
       note bottom of main: this is the main runtime. \n it will parse the arguments you pass via CLI\n and instantiate the Merlin engine.

       /'Merlin'/
       note left of Merlin: here is defined the general engine for running the code. \nCreating an engine allows you to initialize your operations \nand pass to it your arguments of the CLI.
       note right of Merlin: note that the souce is in the run function. \nThere depending on the task selected you select \na different algo for the engine. \nSee for instance merlin::em. 

       /'EM'/
       note left of EM: this runs the EM algorithm. \nThe class inherits form graphical models.
       note left: inherits just cte.h. \nAsk Radu if it just works with cte and not approximate inference.

       /'cte'/
       note right of cte: exact inference method. clique tree. 

       /'observation'/
       note right of observation: there is an observation class that deals with all of the observation types. \nyou have to understand this. \nYou also pass it as type to vector. Can you pass **classes** to vectors? \nNote this is not the only class. There are similar things as well for sets etc. so all defined in there.

   /'Diagram Structure and References'/

       /'Calls'/
       Merlin <|-down- main    
       EM <|-- Merlin

       /'Inheritance'/
       graphical_model .. EM
       cte .. EM
       graphical_model .. factors

   /' Class Functions '/

       /'Merlin'/
       Merlin : set arguments, \ni.e. file of interes, \noutput format etc.
       Merlin : read the passed model in the uai format. \nand extracts the factors of the model.
       Merlin : run() - runs the engine; here s.run() runs the aglo
       Merlin : run() - note that when running the engine there is a check \ntest if there is presence for virtual evidence. \naugments graphical model gm in case of presence.
       Merlin : run() - note that when running the engine you instantiate the inference object \n see for instance **merlin::bte s(fs);** in there.
       Merlin : init() - checks paramters and return fals if smth not working

       /'EM Algorithm'/

   @enduml
   #+end_src
   
   #+RESULTS:
   [[file:./images/strucutre.png]]


** TODO ask radu

   
*** augmenting network with virtual evidence

    I know that I had already asked it... but don't find my
    answer. why do we have two ways to enlarge the network with virtual
    evidence?

    one in the =merlin.cpp= and one in the =em.cpp=?

    what is the difference in there

*** What are locked Factors in the M-step?

    there is the =m_lockedFactors= variable. Do not manage to properly
    make sense of it. which factors should be locked in the M-step?

    this is a variable of type =set= and you apply to it all of the
    operations belonging to that class.

    I guess that this is for some advanced method they used. but it
    might as well be interesting to me as I might work with it at some
    point.
    
*** ask radu if the em just works with cte inference method to this stage.
*** what are m_families?

    the argument there in the code is not the clearest.

    ask to go over together to the piece of code in the em_init algorithm.
*** where do you get if the observation is virtual or not

    cannot understand it at the moment. 

    #+BEGIN_SRC cpp 
    if (obs.is_virtual()) {
      virtualEvidence.push_back(obs); // note push_back is a function of the vector operator. it adds obs at the end of the vecctor. 
    } 
    #+END_SRC

    there you get observations from om entries coming from
    =m_dataset=. so there on =m_dataset= it must be somehow described
    how you would use and work with virtual evidence. 
    
** Reminder of c++

*** Declaration of pointer

    Note that there are two different things you can declare with the
    =&= operator. You should not confuse these. They look similar but
    are not.

    On the one hand you have =references= on the other one you have
    =pointers=.

    These are not the same and should not be confused.

    So on the one hand you have references. These are implemented as
    follows:

    So you have a reference to x.

    #+BEGIN_SRC cpp :libs -std=c++11 -I./my_code_env/include
    #include <iostream>

    int main(){

      float x = 10.7;

      float& rx = x;

      rx = 8;

      printf("the value of the x is: %f ", x);

      return 0;
    }
    #+END_SRC

    #+RESULTS:
    : the value of the x is: 8.000000

    So you see that when you modify =rx= you are actually also
    modifying =x=. This is the entire idea of reference. You have a
    new variable referencing the other one.

    Note then that there are other subtle things you can do. For
    instance passing a reference with =const= such that you can just
    read the referenced variable but you cannot write it itself.

    See for instance the below that would yield an error.

    #+BEGIN_SRC cpp :libs -std=c++11 -I./my_code_env/include
    #include <iostream>

    int main(){

      float x = 10.7;

      const float& rx = x;

      rx = 8;

      printf("the value of the x is: %f ", x);

      return 0;
    }
    #+END_SRC

    #+RESULTS:

    But check that the following works:

    #+BEGIN_SRC cpp :libs -std=c++11 -I./my_code_env/include
    #include <iostream>

    int main(){

      float x = 10.7;

      const float& rx = x;

      x = 8;

      printf("the value of the rx is: %f ", rx);

      return 0;
    }
    #+END_SRC    

    #+RESULTS:
    : the value of the x is: 8.000000

    Check at pointers next.
    
**** Importance of references in c++

     Note that references are especially important in c++ as with it
     you can specify arguments to pass to functions.

     It is actually a fun idea. so you see that there is the
     difference that you do not have to pass a variable do the
     operations and *return* the object at the end of the function
     with the performed operations and finally assign it again to the
     memory. you save some operations in this sense.

     See also the second benefit of passing by reference:

     #+begin_quote
     a function can use the reference parameter to return multiple values to the calling
     function. Passing by value allows only one result as a return value, unless you
     resort to using global variables
     #+end_quote

     Such that it is immediate to see why the above is especially
     important that is straightforward. You can perform operations on
     *Multiple values*
     
     Check for instance the following:

    #+begin_src cpp
    #include <iostream>

    void test( float& a, float& b) { ++a; ++b;}

    int main(){

      float x = 10.7;

      float y = 1.7;
     
      test(x, y);

      printf("the value of the x, y is: %f, %f ", x, y);

      return 0;
    }
    #+end_src

    #+RESULTS:
    | the value of the x | y is: 11.700000 | 2.7 |

    So you see that the above is working as a charm and a is a
    reference to x in the function.

    Note that the return type of a function can also be a referenced
    object.

    Consider the following:

    #+BEGIN_SRC cpp 
string& message() // Reference!
{
static string str = "Today only cold cuts!";
return str;
}
    #+END_SRC

    Then it is immediate to understand that the above would create a
    reference to a static string with the content defined above.

    Then you can also make the referenced objects returned by a
    function read only by passing the =const= operator in the
    following way so to say:

    #+begin_src cpp
const string& message(); // Read-only
    #+end_src

    It is therefore clear and immediate that c++ as a language allows
    you a much richer modeling set.


*** Pointers and Addresses

    So here is the syntax for defining pointers.

    Recall that a pointer is an expression that represents both the
    address and type of another object.

    You can either note that creating the address operator =&= for a
    *given object* creates a pointer to that object.

    So you can for instance get the address of a defined =int var=
    with the following: ~&var~.

    A pointer points to a memory address and simultaneously /indicates
    by its type/ how the memory address can be read or written to.

    You can as well define /pointer variables/. This are used as
    variables to store pointers references.

    See for instance the following to understand this:

    #+begin_src cpp
    int *ptr; // or: int* ptr; // creates a variable to store a pointer to an int.
    #+end_src

    After declaring a pointer variable, you must point the pointer at
    a memory address. The program on the opposite page does this using
    the statement

    #+BEGIN_SRC cpp 
    ptr = &var;
    #+END_SRC

    So once you defined your pointers, as in the following, this is
    generally the syntax you work with

    #+BEGIN_SRC cpp
    #include <iostream>

    int main(){

      double x, y, *px;

      px = &x; // Let px point to x.
      *px = 12.3; // Assign the value 12.3 to x
      *px += 4.5; // Increment x by 4.5.

      printf("the value of the x, px: %f, %f ", x, *px);

      return 0;
    }  
    #+END_SRC

    #+RESULTS:
    | the value of the x | px: 16.800000 | 16.8 |

    So you see that the way you operate with pointers and references
    is the same. What changes is the fact the one is a distinct object
    and the other is not.

    Notice as well the following syntax for pointer declaration:

    #+BEGIN_SRC cpp 
    long *ptr;
    #+END_SRC

    The above essentially means: you create a pointer =ptr= pointing
    to a =long*= i.e. an address with a long value. This is it essentially.

    [[file:images/Bildschirmfoto_2021-03-28_um_16.58.48.png]]

    Note that this is the difference among adress reference &variable
    and pointer. A pointer is a separate object. It can changes
    referenced object. If you declare a reference when initializing a
    variable say =a = &x= you are creating an alias for the object
    x. This reference cannot change at a later point. and the variable
    has not an address in memory that references =a= itself. This is
    different when working with pointers.

    often references are used when declaring functions. these are
    passed as arguments. as you do not have to pass entire objects to
    the function then but rather you point to the objects of interest
    in memory.


*** Passing by pointer - this is a third option apart from passing by reference and value.

    The idea is the following:

    you declare a function parameter to allow an address to be passed
    to the function as an argument.

    you can then do this as follows:

    #+BEGIN_SRC cpp 
#include <iostream>
using namespace std;
void swap( float *, float *); // Prototype of swap()

int main()
{

 float x = 11.1F;
 float y = 22.2F;

 swap( &x, &y );

 printf("value of x: %f \nvalue of y: %f", x,y);

 return 0;

} 

void swap( float *p1, float *p2) // so notice that you pass a pointer
				 // to x, and then this extract the
				 // value in pointer syntax
{
 float temp; // Temporary variable
 temp = *p1; // At the above call p1 points
 *p1 = *p2; // to x and p2 to y.
 *p2 = temp;
}
    #+END_SRC

    #+RESULTS:
    | value | of | x: | 22.200001 |
    | value | of | y: |      11.1 |

    So you can see that this is ultimately extremely close to the
    reference idea in the way it works. It is just an added layer of
    customizing and making your code more granular.    


*** Diff point and references

    References are similar to pointers: both refer to an object in
    memory. However, a pointer is *not merely an alias* but an
    *individual object that has an identity separate from the object* it
    references.

    A pointer has its own memory address and can be manipulated by
    pointing it at a /new memory address/ and thus referencing a
    different object.


*** typedef

    this is a simple way to give a new name to your specified
    objects.

    For instance you might rephrase an =unsigned char= to a =BYTE= by:
    
    =typedef unsigned char BYTE=

    


*** constructors and member initialization functions

    #+BEGIN_SRC cpp
factor(factor const& f) :
  v_(f.v_), t_(f.t_), c_(f.c_) {
};
    #+END_SRC

    This is the /member initializer notation/.

    Understand the initializer notation in the following example:

    #+BEGIN_SRC cpp
class Box {
public:
    // Default constructor
    Box() {} // with no elemnts

    // Initialize a Box with equal dimensions (i.e. a cube)
    explicit Box(int i) : m_width(i), m_length(i), m_height(i) // member init list
    {} 

    // Initialize a Box with custom dimensions
    Box(int width, int length, int height)
        : m_width(width), m_length(length), m_height(height)
    {}

    int Volume() { return m_width * m_length * m_height; }

private:
    // Will have value of 0 when default constructor is called.
    // If we didn't zero-init here, default constructor would
    // leave them uninitialized with garbage values.
    int m_width{ 0 };
    int m_length{ 0 };
    int m_height{ 0 };
};
    #+END_SRC

    The general page for understanding constructors [[https://docs.microsoft.com/en-us/cpp/cpp/constructors-cpp?view=msvc-160][is this]].

    Another example for the constructor is this:

    #+BEGIN_SRC cpp 
class TelList
{
private:
  Element v[MAX]; // The array and the current
  int count; // number of elements
public:
  TelList(){ count = 0;}
}
    #+END_SRC


*** size_t

    this is used everywhere in the code. and I needed to make sense of
    it. turns out that it is a standard library method.

    =std::size_t= can store the maximum size of a theoretically possible
    object of any type (including array). A type whose size cannot be
    represented by =std::size_t= is ill-formed (since C++14) On many
    platforms (an exception is systems with segmented addressing)
    =std::size_t= can safely store the value of any non-member pointer,
    in which case it is synonymous with std::uintptr_t.

    =std::size_t= is commonly used for array indexing and loop
    counting. Programs that use other types, such as unsigned int, for
    array indexing may fail on, e.g. 64-bit systems when the index
    exceeds UINT_MAX or if it relies on 32-bit modular arithmetic.


*** arrays

    #+BEGIN_SRC cpp
#include <iostream>
#include <iomanip>
using namespace std;
int main()
{
const int MAXCNT = 10; // Constant
float arr[MAXCNT], x; // Array, temp. variable so like this you
		      // declare both the array as the temporal
		      // variable x as floats.
int i, cnt; // Index, quantity
cout << "Enter up to 10 numbers \n"
<< "(Quit with a letter):" << endl;
for( i = 0; i < MAXCNT && cin >> x; ++i)
arr[i] = x;
cnt = i;
cout << "The given numbers:\n" << endl;
for( i = 0; i < cnt; ++i)
cout << setw(10) << arr[i];
cout << endl;
return 0;
}
    #+END_SRC

    An array contains multiple objects of identical types stored
    sequentially in memory.


    The definition includes the array name and the type and number of
    array elements.

    An example:

    #+BEGIN_SRC cpp
    int myFirstArray[10]; // Array name
    #+END_SRC

    If you want to initialize the arrays directly when you initialize
    them use the following notation passing a list with the elements:

    #+BEGIN_SRC cpp
    int num[3] = { 30, 50, 80 };
    #+END_SRC

    If the array length is explicitly stated in the definition and is
    larger than the number of initial values, any remaining array
    elements are set to zero.

    Locally defined arrays are created on the stack at program
    runtime. Arrays that occupy a large amount of memory (e.g., more
    than one kbyte) should be defined as global or static.

    you can also use arrays to save objects of a given class. this can
    be done in the following way:

    #+BEGIN_SRC cpp 
    <class_name> myArray[10] // where 10 = dimension.
    #+END_SRC

    Such class arrays can be initialized using class arrays

    #+BEGIN_SRC cpp 
    Result temperatureTab[24] =
    { // this is your class array. containing all of the objects you
      // will save in the array in memory.
    Result( -2.5, 0,30,30),
    Result( 3.5), // At present time
    4.5, //  Instead of using a constructor with one argument, you can
	 //  simply supply the argument. The default constructor is
	 //  then called for the remaining elements.
    Result( temp1), // Copy constructor
    temp2 // Just so
    };
    #+END_SRC


    If the size of an array is not stated explicitly, the number of
    values in the initialization list defines the size of the array.

    The public interface of the objects in the array is available for
    use as usual. I.e. you can call methods in the following way:

    #+BEGIN_SRC cpp 
    temperatureTab[2].setTime( 2,30,21);
    #+END_SRC
    

*** vectors

    Vectors are implemented in the =standard template library=.

    Specifically used to work with dynamic data, C++ vectors *may
    expand depending on the elements they contain*. That makes it
    different from a fixed-size array.

    C++ vectors can automatically manage storage. It is efficient if
    you add and delete data often.

    In C++ vectors, automatic reallocation happens whenever the total
    amount of memory is used.

    The syntax for declaring a vector is the following

    #+BEGIN_SRC cpp 
    vector <type> variable (elements)
    #+END_SRC

    So for instance

    #+BEGIN_SRC cpp 
    vector <int> rooms (9);
    #+END_SRC

    Note that the number of elements is optional. this because as
    mentioned we can enlarge or decrease the size of the vectors at
    runtime.

    To resize a vector to match a given shape - i.e. number of
    elements use the following structure:

    #+BEGIN_SRC cpp 
    rooms.resize(shape)
    #+END_SRC    

    #+RESULTS:

    check at the initializer with =-1= and understand what this =-1=
    is exactly doing:

    #+BEGIN_SRC cpp
   #include <vector>
   #include <iostream>

   int main(){

      std::vector<int> hello(8, -1); // so notice that the second argument is the intializator number for the vector. 

      std::cout << hello[2] << std::endl;

      printf("check at the size of this vector: %d", hello[2]);

      return 0;
    }
    #+END_SRC

    #+RESULTS:
    |    -1 |    |     |      |    |      |         |    |
    | check | at | the | size | of | this | vector: | -1 |


*** templates

    check at [[https://www.youtube.com/watch?v=a-3hcS-tEn0][this video]] for understanding templates. basically it is
    nothing new. you just specify blueprints that you can then call by
    name. the properties are then derived for such a template.


*** conditional operator

    I guess this is as in your javascript notes.

    This basically means if the expression =m_evidence.empty()=
    evaluates to true then return =false= otherwise return =true=

    #+BEGIN_SRC cpp 
   bool plainEvidence = (m_evidence.empty() ? false : true);
    #+END_SRC

    
*** some standard functions

    #+begin_src cpp
    std::copy(m_lockedFactors.begin(), m_lockedFactors.end(), 
	    std::ostream_iterator<int>(std::cout, " "));
    #+end_src

    like this you pass each of the locked factors from begin to end to
    the set to the ostream_iterator that would then cout these.
    


*** linker

    puahh.. I recall that was messy. I have to ask again the pc to
    martina to get back all of my notes and build on that. was quite
    annoying with the linker stuff etc. 

    
** CENTRAL TO UNDERSTAND - WORKING STRUCTURE - graphical_model
   
   from uai  variable create factors vector.

   fixup at the end: from factor creates nodes and edges. (creates the graph).

   you will not be worked with graph - you work with list of factors
   now.

   so here there is the entire flow: from uai to graphical models to
   factors. here are also all of the functions to add factors, remove
   factors etc.


** understand how you pass structure and evidence
   
   So basically the structure on how you pass things is separate and
   well differentiated.

   You pass the network itself with the associated CPT in the =.uai=
   file.

   You pass the evidence on which to update your parameters via the
   =.evid= files.

   Finally you pass the virtual evidence via the following file format
   =.vevid=

   The way you pass the parameters is described in the [[file:merlin/README.md][Readme]].

   so the meat is all here:
   

*** uai format
    :LOGBOOK:
    CLOCK: [2021-03-29 Mon 15:23]--[2021-03-29 Mon 15:49] =>  0:26
    :END:

    to understand the uai format refer to [[https://www.cs.huji.ac.il/project/PASCAL/fileFormat.php][this source]].

    consider now [[file:merlin/data/ChestClinic.uai][this file]]. this is the chestclinic file in the merlin
    project.

    I will discuss the notation of this here once more.

    so there are essentially two sections in this kind of files.

    the first section denotes the structure of the network. then in
    the second you specify the CPT entries.

    so for the first section the situation can look as follows:

    The first integer in each line specifies the number of variables
    in the clique, followed by the actual indexes of the variables.

    #+begin_example
BAYES                 // first line always specify the type of graphical model: bayes or markov
8                     // the number of variables in your model
 2 2 2 2 2 2 2 2      // the number of possible outcomes per variable -> so here all binary
8                     // the number of *factors*
 1 3                  // the first number represents the number of variables per factor - i.e. the scope of the factor
 2 0 1                // so here you have two variables involved for the factor.  
 3 4 2 5              // the other numbers that follow specify which variables are involved for each factor.
 3 1 5 7              // the variables are represented by the numbers, which represent the index of the variables
 2 0 2                // in the 2 2 2 2 .... 2 above. the index starts from 0
 1 0                  // so for instance this represents the first variable above. 
 2 3 4                
 2 5 6                // last entry is the child. the previous are parents in CPT
    #+end_example

    Then in the second part you specify the actual CPD of the
    factors. 

    For the specific case you would have the following:

    #+begin_example
2   // this is the number of entries in the CPT for each factor. 
 0.01 0.99  // this follows the structure above. i.e. the first entry is for the third factor etc.

 [x_4 = 0 is 0.001]

4
 0.6 0.4 0.3 0.7  // have just to understand how these are expressed. here is where the little Endian cicks in.

8
 1.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0

// x_5 = 0, x_3 = 0, x_6 = 0
// x_5 = 0, x_3 = 0, x_6 = 1  // so here you always change the last one and the order stays the same as line 470.
                              // go from right to left.

// then this notation is changed in the factor.h to bigEndian which would be as follows. 

// so here the definition is the following: Tuples are implicitly assumed in ascending order,
// with the *last variable in the scope* as the 'least significant' i.e. the one you change faster. 
// so in the above for instance you have three variables x_5, x_2, x_4. Then you understand that here
// x_5 is the least signigicant. x_2 the most significant.
// this means that for the above you should read it as follows:
// [x_5 = 0, x_4 = 0, x_2 = 0]
// [x_5 = 1, x_4 = 0, x_2 = 0]
// [x_5 = 0, x_4 = 1, x_2 = 0]
// [x_5 = 1, x_4 = 1, x_2 = 0]
// [x_5 = 0, x_4 = 0, x_2 = 1]
// [x_5 = 1, x_4 = 0, x_2 = 1]
// [x_5 = 0, x_4 = 1, x_2 = 1]
// [x_5 = 1, x_4 = 1, x_2 = 1]

8
 0.9 0.1 0.8 0.2 0.7 0.3 0.1 0.9

4
 0.1 0.9 0.01 0.99

2
 0.5 0.5

4
 0.05 0.95 0.01 0.99

4
 0.98 0.02 0.05 0.95
    #+end_example


*** .uai structure is important for developing the graph and pass it along.

    Note that factors are general function mapping from a domain of
    variables (D) to the real numbers. It is therefore a general
    function but it is used in our sense as a map from network
    variables to probability functions. (usually - i.e. if the order is
    meaningful in the sense that the multiplication of factors follows
    the conditional independence structure and factors represent ).

 ///
 /// \brief Factor for graphical models.
 ///
 /// Table based representation of a factor for graphical models. A 
 /// factor encodes a potential (sometimes a probability distribution)
 /// defined over a subset of discrete random variables, called a *scope*, and 
 /// associates each configuration of the variables in the scope with a 
 /// positive real value (sometimes a probability value). The scope is assumed
 /// to be sorted lexicogaphically (e.g., [x1,x2,x3]) Also, the indexing of
 /// configurations in the factor table is assumed to be based on the BigEndian
 /// convention, namely the *first* variable in the ordered scope changes
 /// the fastest, then the *second* variable changes its value and so on.
 /// For example, consider a factor over binary variables [x1,x2,x3].
 /// The corresponding factor table is indexed as follows (internally):
 ///
 /// 0: [0,0,0]    4: [0,0,1]
 /// 1: [1,0,0]    5: [1,0,1]
 /// 2: [0,1,0]    6: [0,1,1]
 /// 3: [1,1,0]    7: [1,1,1]

    factor logic and indexing is different from the =.uai= representation
    and this is described above.

    Note that it is important this piece of code:

    #+begin_example
   The scope is assumed to be sorted lexicogaphically (e.g., [x1,x2,x3])
    #+end_example

    Also, the indexing of configurations in the factor table is assumed to
    be based on the BigEndian, namely the *first* variable in the ordered
    scope changes the fastest, then the *second* variable changes its
    value and so on.

    I.e. for each factor you have a /factor table/ that maps your Val(D)
    to real line. In this table there are all of the possible combinations
    of Val(D). The question is then on how you keep record of these and
    the solution is the BigEndian notation.

    There is a function *convert_index*  - maybe not a function have to
    understand that tomorrow. the syntax is not the one of a
    function.
   
    These are in fact both classes that are defined in this file
    [[file:merlin/include/index.h]].

    So notice that this conversion is done because of the following
    reason:

    #+begin_example
   // BigEndian assumes that the first variable changes the fastest
   // UAI input is assumed to follow the LittleEndian convention, whereas
   // the internal representation of the factors assume BigEndian.
    #+end_example

**** Note that in the factor header also all of the functions for factor summation, entropy etc. are defined.



    
    

    
*** TODO understand the big-endian to little endian trasformation in the c++ document.
    :LOGBOOK:
    CLOCK: [2021-05-29 Sat 15:03]--[2021-05-29 Sat 15:28] =>  0:25
    :END:

    check at the code described above. paste the code and make small
    experiments to be sure you understand this.

    then given that you understand all of this, you can create files
    as in the =.uai= notation where you can pass the hyperparameters
    for each node.

    quite complex piece of code.

    work in the following alternative way:

    (i) start from the em algorithm.. understand all of the sequence
    of functions that are called. understand just that code and forget
    about the rest. use a reverse engineering technique in this sense.


*** .evid

    Evidence is specified in a separate file. This file has the same
    name as the original network file but with an added =.evid=
    suffix. For instance, problem.uai will have evidence in
    =problem.uai.evid=.

    the syntax is the following:

    #+begin_example
1 // first line => number of evidences samples
2 1 0 2 1 // evidence in each sample, will be written in a new line. first entry = number of observed variables.
          // then pairs. (<variable>, <value>) 
    #+end_example

    So in the example above you would specify that you observe just
    two observations x_2 and x_3 (recall that indexing starts at 0).

    where x_2 = 0, x_3 = 1.
    

*** .vevid

    same idea here. same structure just you have likelihoods instead
    of plain observations.

    see for instance for the specific project the following:

    #+begin_example
    2  // number of evidence
    1 2 0.6 0.8  // first entry = variable index. second entry = size of domain of variable. other entries 0 likelihoods
    2 2 0.1 0.3
    #+end_example



*** also in this sense.. how is the flow evidence -> parameters -> uai.

    cause theoretically this is how you would work. note the following
    solution and interpretation.

    apparently you need both. then you have the =--init-factors= entry
    to overwrite the parameters that you are interested in and are in the
    =.uai= file (i.e. you can initialize them either uniformly or
    randomly).

    if you do not overwrite I guess that the parameters of the =uai=
    file are just taken as the initializers.

    this is in fact how it works. with the new_thetas. in the em
    algorithm that you compute and then pass to a new graphical_model
    object instantiation.
    


*** training data

    what is the difference between training data and evidence files?

    evid files used for inference. train for parmaters.




   
** Parameter for the algorithm

      #+begin_example
   "Order=MinFill" << ","
   << "Infer=CTE" << ","
   << "Iter=" << m_iterations << ","
   << "Debug=" << (m_debug ? "1" : "0") << ","
   << "Threshold=" << m_threshold << ","
   << "Init=" << initMethod;
   #+end_example

   arguments for instantiation the EM.

   - he has a stopping criteria check at each iteration.
   
     
** wmb bucket

   best approximate inference algorithm.


** note that factors are key not graphs 

   he said that the graph representation in the code is there but is
   not actually used.

   apparently you convert everything into factor format and then work
   from there.

   there is also this twist that he mentioned in the factor
   interpretation there. check at this file [[file:merlin/include/factor.h]]



** General function for hyperparameters
   :properties:
   :hearder-args:cpp: :session hello
   :end:

   Note that like this it works. You can save functions in headers and
   get them from there by specifying the =-I= option.

   #+begin_src cpp :libs -std=c++11 -I./my_code_env/include
#include <vector>
#include <iostream>


#include "hello.h"

int main(){

  std::vector<int> input = { 1, 2, 3, 4, 5 };

  // so it is correct yourename the variable later
  input = { 1, 2, 3, 4, 5, 6, 7, 8 };  
  
  print(input);

  return 0;    
}
   #+end_src   

   #+RESULTS:
   : 1 2 3 4 5 6 7 8

   So that is basically it now you have to embed it in the code. This
   would be a vector containing the prior hyperparameters, starting
   with the bayesian learning MAP estimator.

   Following the syntax of the code base you should write a function
   of the following shape

   #+begin_src cpp
bool Merlin::read_hyperparameters(const char* filename) {
	try {

		// Read the graphical model
		m_filename = std::string(filename);
		std::ifstream is(filename);
		if (is.fail()) {
			std::string err_msg("Cannot open the input file: ");
			err_msg += std::string(filename);
			throw std::runtime_error(err_msg);
		}
		

                // have to specify and read the input out of the .txt file
		// std::vector<int> input = { 1, 2, 3, 4, 5 };

		return true;
	} catch (const std::runtime_error& e) {
		std::cerr << e.what() << std::endl;
		return false;
	}
}
   #+end_src

   Then adapt the M-step. Keep everything equal. Just use a different
   function for the maximization step.
   

** How to save the things in the files

   I decided to save the input of the hyperparameters in a =.prio=
   file.

   There you will have to specify the hyperparameters for each
   node(?). Check at the theory again. And understand what you have to
   use.
   
*** TODO define the structure of such a file
   

** Otherwise you can also set it at parameter at the beginning

   See for instance the =set_init_factor_method= function in the
   =merlin.cpp= file.

   And also all of the other parameters that you set when running the
   merlin engine.

   See =main.cpp=.

   Nonetheless, I think it makes better sense to focus on the file
   solution as there might be quite some parameters if you need
   hyperparameters for each node.
   

** TODO Understand the reading process out of the files.

   There is a function in the =graphical_model.h= file. There the
   =read= function is specified.

   Notice that there you pass as a parameter the =is=. Have to
   understand what that exactly is.

   This is a file =istream=. So I think it is a standard input
   =std::istream&= element.

   Notice that you consume a line of the input stream with =>>= each
   time.

   Try to double check that

   #+BEGIN_SRC cpp :libs -std=c++11 -I./my_code_env/include
#include <iostream>
#include <sstream>
#include <fstream>

void read(std::istream& is, bool positive_mode = false) {

		// Read the header
                bool m_markov;
		size_t nvar, ncliques, csize, v, nval; // here the
						       // type is
						       // size_t. so
						       // you do not
						       // define the
						       // type but you
						       // touch it so
						       // to say,
						       // storing the
						       // maximum
						       // possible
						       // amount of
						       // memory for
						       // that object
						       // in memory.
		std::string st;
		
		is >> st; 
		if ( st.compare("MARKOV") == 0 ) {
		  m_markov = true;
		} else if ( st.compare("BAYES") == 0 ) {
		  m_markov = false;
		} else {
		  std::string err_msg("Merlin only supports the UAI Markov or Bayes file format.");
			throw std::runtime_error(err_msg);
		}

		printf ("%d \n", m_markov);

   }


   int main(){

     std::ifstream file ("./merlin/data/multinomial_dirichlet.prio");

     bool m_positive = true; 
     
     read(file, m_positive);

     return 0;
     
   }
   #+END_SRC

   #+RESULTS:
   : 0

   Good so you understand now how to read file.

   You can then expand based on this.

   Should be fine in any case. You just need to understand the order
   through which you understand the input parameters.

   I.e. with the factor structure etc. Go over it tomorrow morning.

   Then basically do the same structure for passing the
   hyperparameters.

   Then just adapt m-step and boom! You are good to go. 
   
      


** TODO understand the em-algorithm

   Note that for the algorithm you will need three components for
   which you have to specify properties/ways:

   - factor initialization

   - inference algorithm

   - m-algorithm (i.e. threshold for convergence , order etc.)

   There are then three main functions in the code.

   One for initializing the algorithm.

   One for running the e-step.

   One for running the m-step.

   One for running the algorithm until convergence.

   We will check all of them in turn next.

   
*** Run the Algorithm

    So very easy. Just calls the two other methods sequentially and
    stops when the algorithm converged.
    
*** Initialize the Algorithm

    so at first checks at the missing and virtual evidence and count
    how many observations of these are present as well as their share
    amount.

    then you check which factors are locked. so probably they already
    started to work on something with locked factors.

    then you initialize the CPTs. you can either do this via uniform
    or random method.
    
    #+begin_src cpp
    	// Initialize the CPTs uniformly at random
	if (m_init_method == InitMethod::Uniform) {
		m_gmo.uniform_bayes(m_lockedFactors);
	} else if (m_init_method == InitMethod::Random){
		m_gmo.random_bayes(m_lockedFactors);
	}
    #+end_src

    so notice that this is exactly the step that you will be *required
    to change*.

    then there is a section about the initialization of the junction
    tree.

    then it follows a bayes net initialization phase. that is quite
    interesting. it talks about families and that is a bit confusing
    to me but I guess that there is where you initialize everything
    together with the initialization of the CPTs above. 

    
    
*** M-step

    
    
*** E-step



    
** General Notes
   
*** Recall that in Pearls method the extended virtual node is always set to true.

*** IMPORTANT: note that the likelihood ratios are passed normalized in the IBM paper... so that they in fact represent probabilities P(obs | x_i)

*** Linear Gaussian Bayesian Networks

    linear
    Gaussian CPDs the local likelihood is given by:


    Performing this task you would have for the log-likelihood of
    linear Gaussian Bayesian:
    
    #+begin_export latex
    \begin{align} \label{eq:like-gaussian-cpd}
    P(X|\theta) = \sum_m -log(\sqrt{2\pi\sigma^2}) \sum_{h[m] \in Val(\mathscr{H}[m])} Q(h[m]) * - \mathbf{\theta}^\intercal \mathbf{\tau(d[m], h[m])}
    \end{align}
    #+end_export            

    Such that

     
**** TODO TO SOLVE THAT PART WORK AS FOLLOWS

***** TODO Look at the chapter 19

      there you can see how the same step of expectation is valid for
      the case of exponential families. there is also clear what the
      sufficient statistics are - recall your course of mathematical
      statistics.

      apparently you use the very same sufficient statistics for the
      exercise of performing your Map projection - see first point
      here above.

      so you get this fish and you can most likely write here the
      theory for the general exponential family. then just a matter of
      generalizing the above.

      there the idea is that you can compute expected sufficient
      statistics, instead of simple sufficient statistics in the very
      same way, i.e. through the posterior P(H | O, \theta_{current}).

      then you can use these when computing the M-step.

      so it is the same story. have just to understand better the
      story of the expected sufficient statistics and how it is
      defined in 17. especially for the empirical distribution and
      not. then understand the inverting process. and from there it
      should be clear.

      the confusing point to this stage is that you will have two
      expected steps which I do not think will be the same.

      I would suggest the following:

      1. compute the MLE for a standard case.

      2. understand what is the sufficient statistics in there.

      3. understand how you can compute the ess from there.

      4. relate everything and make sense of it also in conjunction
         with the piece of information from the chapter 17 in here.



    \newpage

   


   
