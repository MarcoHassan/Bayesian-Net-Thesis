#+begin_export html
<style>
img {
display: block;
margin-top: 60px;
margin-bottom: 60px;
margin-left: auto;
margin-right: auto;
width: 70%;
height: 100%;
class: center;
}
</style>
#+end_export


Here are some Notes about the topic of my Master Thesis - Bayesian
Networks.

Note that most of these Notes are based on /Probabilistic Graphical
Models - Principles and Techniques/ ([[https://www.amazon.de/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193][Koller and Friedman]]).

* Bayesian Networks
  :LOGBOOK:
  CLOCK: [2021-02-15 Mon 10:57]--[2021-02-15 Mon 11:22] =>  0:25
  :END:
  
The general outlook.

So recall that in general you have three elements in Bayesian
Networks:

- Representation

  - how do you represent the joint probability of the events as a
    network (i.e. as a graph data structure)? Can such structure
    represent the joint in a compact way due to the conditional
    independence relations?

    *Note 1:* that such compact formulation is one of the key benefits of
    Bayesian Networks as it really gives the possibility of shrinking
    the amount of parameters needed to describe the full joint
    probability leveraging the independence structure among the RVs.

    *Note 2:* this formulation is /transparent/, i.e. highly
    understandable also to non-AI experts. It is so to say highly
    explainable and in this new buzz of /explainable AI/ a solid
    option.
  
- Inference

  - given some information about some Parent variables, how can I
    infer/compute the distribution of the children in the Network?
  
- Learning

  - given some observed data, how can I use such information to
    construct / (infer) / learn the  /structure/ of the network?

  - given some observed data, how can I learn the /parameters/ of the
    network? I.e. how can I use the information content of the data to
    derive some plausible parameterization of the network.


So these are the main tasks you have to deal with in Bayesian
Networks. Basically you can do all of the three in a very simple way,
which is from a theoretical standpoint very concrete and
straightforward or you can start to consider all the aspects of the
problem going quickly towards more complex situations.


** Inference

   An important exercise for inference is to query
   distributions. I.e. the task is to compute

   $$

   *Continued on Blog*.






** HELP


   #+begin_export latex
\algrenewcommand\algorithmicindent{1.5em}%

\begin{algorithm*}[h!]
\caption{EM-Proabilistic: an EM algorithm for learning with probabilistic evidence}
\label{alg:EM-Probabilistic}
%\begin{\algsize}
\vspace{-10pt}
\begin{multicols}{2}
\begin{algorithmic}[1] 
\Require Bayesian network $\mathcal{B}=\langle \mathbf{X},\mathbf{D}, G, \mathbf{P} \rangle$, dataset $S$ 

\Procedure{EM}{$\mathcal{B}$, $S$}
\State Initialize $\mathcal{B}$'s parameters $\theta \leftarrow \theta^0$
\ForAll{$t=1, \ldots$ until convergence}
  \State $M-step \ as \ in \ Algorithm \ 1$

\\
\Function{Compute-ESS}{$\mathcal{B}=(G, \theta)$, 

\State Run IPFP given current parameterization
\State $Q_k \leftarrow$ {Return convergence distribution of algorithm IPFP above} \Comment {Note that you have to perform such iteration at each iteration - very expensive}

\ForAll {$i\in1,\ldots,n$}
  \ForAll {$x_{i},u_{i}\in Val(X_{i},Pa_{X_{i}}^{\mathcal{B}})$}
   \State $\bar{M}[x_{i},u_{i}]\leftarrow 0$
  \EndFor
\EndFor

% \State (Go over all evidence nodes, creating an augmented network
% for each one, and collect all of the evidence for the nodes in $G$)
\ForAll{example $S_{j}\in S$}

    \State Run inference on $(G, Q_k, \theta)$ with evidence $d_{j}$

    \ForAll{i$ = 1,\ldots,n$}
      \ForAll{$x_{i},u_{i}\in Val(X_{i},Pa_{X_{i}}^{\mathcal{B}})$}

        \State $\bar{M}[x_{i},u_{i}] \mathrel{{+}{=}} Q_{(G,\theta)}(x_{i},u_{i}|d_{j})$ \Comment {Note that inference is based on the adjusted distriution Q obtained above}
      \EndFor
    \EndFor
\EndFor
\EndProcedure

\end{algorithmic}
\end{multicols}
%\end{\algsize}
\end{algorithm*}
   #+end_export

   
