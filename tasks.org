* [[elisp:(org-projectile-open-project "Bayesian_Net_Thesis")][Bayesian_Net_Thesis]] [4/7]
  :PROPERTIES:
  :CATEGORY: Bayesian_Net_Thesis
  :END:


** TODO for next week


*** TODO write the numerical part of the M-step into the algorithm

    check [[http://www.math.niu.edu/~dattab/math435/LectureNotes.pdf][this document]]. note that you can also use the secant method
    instead of the newton method. then you do not have to compute the
    numerical derivative. there you do not have to compute the
    derivative. the convergence is slower - superlinear instead of
    quadratic.
    
    have to investigate the task of computing the numerical derivative
    of Q_1 in the thesis. how can you do that? can you simply use
    central numerical derivative for that? and two times that for the
    hessian?

    -> all open questions you need to solve at some point.

    *important note:* notice that when computing Q_1 in the numerical
    derivative, this is nothing else than the score or derivative of

    #+begin_export latex
    \sum_h P(h | \mathscr{D}, \theta_0) * \frac{\partial}{\partial \theta} l (\theta: \mathscr{D}, \mathscr{H})\\
    #+end_export

    where such derivative is performed at \theta_0.

    also note that the above is nothing more than the score and
    information criteria of such log-likelihood. can use plug-in
    estimators. - no need to go with the secant method nor to compute
    the numerical derivative. this would be a killer.

    Double check and add this to the document. Should be correct in
    any case.


    -- note that at the end also score and information criteria
    requires derivative. so not that sure that there is a big benefit
    doing so. so actually very poor method. would need to compute the
    gradient and hessian for each all of the possible completion.
        
    
*** TODO think of a network example

    think about a network structure where you can apply the
    above. check at this link radu sent you with all of the


*** TODO try with a different distribution. i.e. different CPT distribution and prior

    this is an ask of Radu. maybe prioritze shortly. go for instance
    with the exponential family. this is already very straightforward
    in the book and a generalization with a conjugate prior should be
    of no difficulty.

    
    
** DONE understand why global decomposition with complete data and why not with missing
   CLOSED: [2021-04-02 Fri 17:16]
   :PROPERTIES:
   :WILD_NOTIFIER_NOTIFY_BEFORE: 30 10 5
   :END:
   :LOGBOOK:
   CLOCK: [2021-04-02 Fri 14:58]--[2021-04-02 Fri 15:23] =>  0:25
   CLOCK: [2021-04-02 Fri 14:27]--[2021-04-02 Fri 14:52] =>  0:25
   CLOCK: [2021-04-02 Fri 12:36]--[2021-04-02 Fri 12:41] =>  0:05
   CLOCK: [2021-04-02 Fri 12:11]--[2021-04-02 Fri 12:36] =>  0:25
   CLOCK: [2021-04-02 Fri 11:20]--[2021-04-02 Fri 11:29] =>  0:09
   CLOCK: [2021-04-02 Fri 10:16]--[2021-04-02 Fri 10:41] =>  0:25
   CLOCK: [2021-04-02 Fri 09:45]--[2021-04-02 Fri 10:10] =>  0:25
   CLOCK: [2021-04-02 Fri 08:58]--[2021-04-02 Fri 09:23] =>  0:25
   :END:

   Check at the mind map.


** DONE solve exercise 19.20 to understand how EM generalizes to the case of MAP bayesian estim.
   CLOSED: [2021-04-03 Sat 16:17] SCHEDULED: <2021-04-03 Sat 10:00>

   Pretty simple in the end. See tablet

*** DONE reformulate and write down everything in a neat way to prepare for the discussion with Radu
    CLOSED: [2021-04-11 Sun 17:59] SCHEDULED: <2021-04-06 Tue 18:00>

*** TODO understand what are the assumptions for the above? global decomposition? etc.? (have to specify MAR property for it).
*** DONE start to write this piece as for the thesis. Like this you can show it to Radu and you will have it ready for the thesis.    
    CLOSED: [2021-04-11 Sun 17:59]
    
*** DONE well behaved condition?
    CLOSED: [2021-04-03 Sat 16:31]

    can take derivative so that it is sufficiently easy to compute the
    argmax of the sum of the two.


** DONE understand if it does not break anything of the EM as in the paper
   CLOSED: [2021-04-03 Sat 17:23] SCHEDULED: <2021-04-04 Sun 08:30>

   this should be the case. we should be done like this. => just need
   to implement it then.

   => it does not to the best of my understanding so you should be
   fine. You can easily extend the EM-algorithm to the Bayesian
   Setting.
   

** DONE use template of other Thesis for the new one.



** TODO add the algorithm for plain EM and from there sequentially increment algorithm throughout the thesis

** TODO notice that at the end there is this nice thing.

   - likelihood evidence alters the e-step. cause it alters the way you
     do the inference step in your network.

   - map estimation and bayes prior changes the M-step

   so taking the two together you will have both an updated E and M
   step.


** WAITING other possible generalizations
   
*** TODO add jeffery's method and pearl method into the mind map
    SCHEDULED: <2021-05-03 Mon 10:00>
    :PROPERTIES:
    :WILD_NOTIFIER_NOTIFY_BEFORE: 30 10 5
    :END:

     - Investigate the relations: i.e. when you are under likelihood
       evidence you propagate - the inference step is equal.

     - when you are under probabilistic evidence you set limits to the
       extent you propagate. i.e. it depends how new information will
       flow in  the inference step as there the 5essential idea is that
       you express already a probability - all else considered.

*** TODO consider at some point the following enlargement of the EM algorithm (at least mention it in the thesis).

     - https://www.math.kth.se/matstat/gru/Statistical%20inference/Lecture8.pdf

*** TODO consider as well approximate inference

     - Provare anche approximate inference for the above. Should work in
       a similar way.

     - Also here at the theoretical level nothing has changed.

*** TODO consider how non-informative prior would affect the whole thing
*** DONE think about performing M-step in numeric way - how are the properties of EM affected
    CLOSED: [2021-04-18 Sun 11:00]

    like this you would be able to work theoretically with any prior.

    would have to guarantee the property of increased likelihood at
    each iteration. how can you do that?

    check [[https://www.mn.uio.no/math/tjenester/kunnskap/kompendier/num_opti_likelihoods.pdf][this]] quickly tomorrow.

*** DONE consider to introduce the EM algorithm as in her
    CLOSED: [2021-04-18 Sun 11:01]

    https://escholarship.org/content/qt2wm4j93p/qt2wm4j93p.pdf


    at the very beginning. Very well written with expectation part in
    general terms.

    

    
    
       
