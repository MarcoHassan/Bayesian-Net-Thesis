* [[elisp:(org-projectile-open-project "Bayesian_Net_Thesis")][Bayesian_Net_Thesis]] [4/7]
  :PROPERTIES:
  :CATEGORY: Bayesian_Net_Thesis
  :END:


** TODO for next week

*** TODO think of a network example

    think about a network structure where you can apply the
    above. check at this link radu sent you with all of the


** DONE understand why global decomposition with complete data and why not with missing
   CLOSED: [2021-04-02 Fri 17:16]
   :PROPERTIES:
   :WILD_NOTIFIER_NOTIFY_BEFORE: 30 10 5
   :END:
   :LOGBOOK:
   CLOCK: [2021-04-02 Fri 14:58]--[2021-04-02 Fri 15:23] =>  0:25
   CLOCK: [2021-04-02 Fri 14:27]--[2021-04-02 Fri 14:52] =>  0:25
   CLOCK: [2021-04-02 Fri 12:36]--[2021-04-02 Fri 12:41] =>  0:05
   CLOCK: [2021-04-02 Fri 12:11]--[2021-04-02 Fri 12:36] =>  0:25
   CLOCK: [2021-04-02 Fri 11:20]--[2021-04-02 Fri 11:29] =>  0:09
   CLOCK: [2021-04-02 Fri 10:16]--[2021-04-02 Fri 10:41] =>  0:25
   CLOCK: [2021-04-02 Fri 09:45]--[2021-04-02 Fri 10:10] =>  0:25
   CLOCK: [2021-04-02 Fri 08:58]--[2021-04-02 Fri 09:23] =>  0:25
   :END:

   Check at the mind map.


** DONE solve exercise 19.20 to understand how EM generalizes to the case of MAP bayesian estim.
   CLOSED: [2021-04-03 Sat 16:17] SCHEDULED: <2021-04-03 Sat 10:00>

   Pretty simple in the end. See tablet

*** DONE reformulate and write down everything in a neat way to prepare for the discussion with Radu
    CLOSED: [2021-04-11 Sun 17:59] SCHEDULED: <2021-04-06 Tue 18:00>

*** TODO understand what are the assumptions for the above? global decomposition? etc.? (have to specify MAR property for it).
*** DONE start to write this piece as for the thesis. Like this you can show it to Radu and you will have it ready for the thesis.    
    CLOSED: [2021-04-11 Sun 17:59]
    
*** DONE well behaved condition?
    CLOSED: [2021-04-03 Sat 16:31]

    can take derivative so that it is sufficiently easy to compute the
    argmax of the sum of the two.


** DONE understand if it does not break anything of the EM as in the paper
   CLOSED: [2021-04-03 Sat 17:23] SCHEDULED: <2021-04-04 Sun 08:30>

   this should be the case. we should be done like this. => just need
   to implement it then.

   => it does not to the best of my understanding so you should be
   fine. You can easily extend the EM-algorithm to the Bayesian
   Setting.
   
** DONE use template of other Thesis for the new one.


** TODO add the algorithm for plain EM and from there sequentially increment algorithm throughout the thesis

** TODO notice that at the end there is this nice thing.

   - likelihood evidence alters the e-step. cause it alters the way you
     do the inference step in your network.

   - map estimation and bayes prior changes the M-step

   so taking the two together you will have both an updated E and M
   step.

** WAITING other possible generalizations
   
*** TODO add jeffery's method and pearl method into the mind map
    SCHEDULED: <2021-04-16 Fri 10:00>
    :PROPERTIES:
    :WILD_NOTIFIER_NOTIFY_BEFORE: 30 10 5
    :END:

     - Investigate the relations: i.e. when you are under likelihood
       evidence you propagate - the inference step is equal.

     - when you are under probabilistic evidence you set limits to the
       extent you propagate. i.e. it depends how new information will
       flow in  the inference step as there the 5essential idea is that
       you express already a probability - all else considered.

*** TODO consider at some point the following enlargement of the EM algorithm (at least mention it in the thesis).

     - https://www.math.kth.se/matstat/gru/Statistical%20inference/Lecture8.pdf

*** TODO consider as well approximate inference

     - Provare anche approximate inference for the above. Should work in
       a similar way.

     - Also here at the theoretical level nothing has changed.

*** TODO consider how non-informative prior would affect the whole thing
*** DONE think about performing M-step in numeric way - how are the properties of EM affected
    CLOSED: [2021-04-18 Sun 11:00]

    like this you would be able to work theoretically with any prior.

    would have to guarantee the property of increased likelihood at
    each iteration. how can you do that?

    check [[https://www.mn.uio.no/math/tjenester/kunnskap/kompendier/num_opti_likelihoods.pdf][this]] quickly tomorrow.

*** DONE consider to introduce the EM algorithm as in her
    CLOSED: [2021-04-18 Sun 11:01]

    https://escholarship.org/content/qt2wm4j93p/qt2wm4j93p.pdf


    at the very beginning. Very well written with expectation part in
    general terms.

    

    
    
       
