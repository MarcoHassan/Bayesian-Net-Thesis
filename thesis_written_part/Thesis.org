#+LATEX_CLASS: article
#+LATEX_HEADER: \usepackage{arxiv}
#+OPTIONS: toc:nil

#+begin_export latex
\newtheorem{theorem}{Theorem}

\title{Parameter Learning in Bayesian Networks under Uncertain Evidence  \textendash  \ An Exploratory Research.}
\author{
  Marco Hassan 	           	\\
  Zurich, CH		\\
  \\
  \\
  Master Thesis \\
  Presented to the Eidgenossische Teschnische Hochschule Zurich \\
  In Fulfillment Of the Requirements for \\ 
  the Master of Science in Statistics \\
  \\
  Supervisor: PhD. Radu Marinescu \\
  Co-Supervisor: Dr. Markus Kalisch \\
  %% examples of more authors
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\   
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{article}

\maketitle
#+end_export

\newpage

\tableofcontents

\newpage

* TODO Bayesian Networks Overview and Definition & Literature Review
  
* TODO Learning under Complete Evidence

* TODO Types of Uncertain Evidence
  
* TODO on the nastiness of the Likelihood function

  all good properties are gone when working with partially observed
  data. write down why here. 


* The Mathematics of the EM
  :PROPERTIES:
  :CUSTOM_ID: math_em
  :END:

  
  As discussed by cite:koller2009probabilistic it is possible to frame
  the EM as a coordinate ascent optimization of an energy function we
  will define next. Given such perspective we will be able to prove the
  following theorem

  #+begin_export latex
  \begin{theorem}\label{thm:one}
  Write here formally that the likelihood improves at each iteration step
  \end{theorem}
  #+end_export

  Consider the following energy function:

  #+begin_export latex
  \begin{equation} \label{eq:energy_functional}
  F[P, Q] = E_Q[log (\tilde{P})] + H_Q (X)
  \end{equation}
  #+end_export

  Where $\tilde{P}$ is an unnormalized state probability $P =
  \frac{\tilde{P}}{Z}$ and $H_Q$ is the entropy of the observed
  particles. 

  Using such energy functional [[ref:eq:energy_functional]] it is possible
  to re-express the logarithm of the normalizing constant $Z$ as
  follows:

  #+begin_export latex
  \begin{equation} \label{eq:energy_refurmolation}
  log (Z) = F[P, Q] + D (Q||P)
  \end{equation}  
  #+end_export

  where $D(Q||P)$ is the Kullbackâ€“Leibler divergence, or relative
  entropy.

  We will choose next the following distribution for the particle
  distribution:

  #+begin_export latex
  \begin{equation} \label{eq:particle_distribution}
  P (\mathscr{H} | \mathscr{D}, \theta) =   \frac{P (\mathscr{H}, \mathscr{D}| \theta)}{P (\mathscr{D}| \theta)}
  \end{equation}
  #+end_export

  With this choice it becomes clear that $Z (\theta) = P (\mathscr{D}|
  \theta)$ and $\tilde{P} = P (\mathscr{H}, \mathscr{D}| \theta)$ such
  that $\tilde{P}$ represents the likelihood function of $\mathscr{H},
  \mathscr{D}$ and $Z (\theta)$ the likelihood function of
  $\mathscr{D}$:
  
  #+begin_export latex
  \begin{align} \label{eq:likelihood_particle}
  \mathscr{L} (\theta: \mathscr{D}, \mathscr{H}) =& \ P (\mathscr{H}, \mathscr{D}| \theta)\\
  \mathscr{L} (\theta: \mathscr{D}) =& \ P (\mathscr{D}| \theta)
  \end{align}
  #+end_export

  Such that using [[ref:eq:energy_refurmolation]]:

  #+begin_export latex
  \begin{align} \label{eq:likelihood_energy_functional_relation}
  l (\theta: \mathscr{D}) =& \ F_D[\theta, Q] + D (Q (\mathscr{H}) || P (\mathscr{H}| \theta, \mathscr{D})) \\
  l (\theta: \mathscr{D}) =& \ E_Q[l (\theta: \mathscr{D}, \mathscr{H})]+ H_Q (\mathscr {H}) + D (Q (\mathscr{H}) || P (\mathscr{H}| \theta, \mathscr{D}))
  \end{align}
  #+end_export  

  The above are two fundamental equations. It is in fact
  straightforward to see that as both the relative entropy as well as
  the entropy are non-negative the log-likelihood on the left hand
  side above is lower bound for the energy functional and the expected
  log-likelihood relative to Q, for any choice of Q.

  Moreover it is straightforward to see in the above that choosing the
  Q-measure as $P (\mathscr{H}|\mathscr{D}, \theta)$ the relative term
  fades away such that the entropy measures the entropy term that in
  such case is the overall measure on the difference between the
  expected log-likelihood and the real log-likelihood. It is in fact
  clear that in such a case the log-likelihood and the energy
  functional are the one and the same thing.

  In this sense the relation between the energy functional and the
  log-likelihood is clear and we can think of the EM-algorithm as a
  coordinate ascent optimization of the energy functional. To see this
  consider the E-step and M-step as follows.

** The Expectation Step

   Consider the first coordinate ascent - Q, keeping $\theta$
   fixed. We look for $\operatorname*{argmax}_{Q} F_D[\theta, Q]$. It
   is then immediate that:

   #+begin_export latex
   \begin{align} \label{eq:q_optimum}
   Q^* =& \ P (\mathscr{H}|\mathscr{D}, \theta) \\
   F_D[\theta, Q^*] =& \ l (\theta: \mathscr{D}) \\
   F_D[\theta, Q^*] \geq& \ F_D[\theta, Q]
   \end{align}
   #+end_export   

   The reasoning on why the above is actual the searched maximum
   argument is the following: You have in general an upper bound on the
   energy functional given by log-likelihood. If you now choose the
   distribution Q in the way described above you know that you have
   reached the lower bound and that such upper bound is tight. I.e. it
   is straightforward to see that your are at the maximum for a given
   \theta.

   Note that choosing $Q^*$ you are in fact choosing the probability
   density by which you are going to weight the synthetically created
   complete data sets in your E-step, so that you can in fact
   interpret the E-step as the step involving the maximization of the
   energy functional along the Q coordinate.

** The Maximization Step

    This is the second coordinate ascent - \theta. Here we look
    towards $\operatorname*{argmax}_{\theta} F_D[\theta, Q]$.

    It follows then the following quoting from
    cite:koller2009probabilistic:

    "Suppose Q is fixed, because the only term in F that involves \theta is
    E_Q[l (\theta: \mathscr{D}, \mathscr{H})], the maximization is
    equivalent to maximizing the expected log-likelihood."

    It follows now that given the linearity of expectation it is
    possible to take the expectation of the sufficient statistics and
    maximizing for \theta.

    This is in fact exactly the standard M-step of the EM algorithm so
    that we can interpret the M-step as the coordinate ascent along
    the second axis.
    
   Summarizing, by the fact that at each step the energy functional is optimized
   such that it increases it follows from the proposition above
   [[ref:eq:likelihood_energy_functional_relation]] that the log-likelihood
   increases such that theorem [[ref:thm:one]] is proved.


* Bayesian Parameter Learning

  A natural question that arises is whether it is possible to
  generalize the extended algorithm proposed by cite:Mrad_2015 to the
  case of Bayesian Parameter Learning.

  Recall that in Bayesian statistics rather than treating the
  parameters of interest as fixed but unknown you treat them as random
  variables themselves.

  You would then specify a prior, i.e. a probability distribution, for
  the data governing process of the parameters. This can be either a
  non-informative prior or a prior based on your domain knowledge
  expertise.

  Such prior distribution would then be updated upon the arrival of
  new observations according to the well known Bayes Rule. The result
  is an updated posterior distribution from which you can compute your
  statistics of interest.


  #+begin_export latex
  \begin{equation} \label{eq:bayes_formula}
  P (\theta | \mathscr{D}) = \frac{P (\mathscr{D} | \theta) * P(\theta)}{P (\mathscr{D})} 
  \end{equation}
  #+end_export

  It is straightforward to see that that the posterior is proportional
  to a likelihood term $P (\mathscr{D} | \theta)$ multiplied by the
  prior distribution.

  It is clear then, that depending on how you want to leverage the
  information of your posterior you would require a different
  mathematical exercise. I.e. in case you want to use as your
  parameterization of choice the expected value you would need an
  integration exercise and similar reasonings can be done for the
  other metrics.

  Another way you can set your parameters based on the posterior is by
  choosing the most likely parameterization. This is the maximum a
  posteriori parameterization and is defined in mathematical terms as
  follows:

  #+begin_export latex
  \begin{align} 
  \tilde{\theta} =& \operatorname*{argmax}_{\theta} \frac{P (\mathscr{D} | \theta) * P(\theta)}{P (\mathscr{D})} \nonumber\\
  \tilde{\theta} =& \operatorname*{argmax}_{\theta} P (\mathscr{D} | \theta) * P(\theta)\\ \label{eq:bayes_map}
  \tilde{\theta} =& \operatorname*{argmax}_{\theta} log (P (\mathscr{D} | \theta)) + log (P(\theta)) \nonumber \\
  \nonumber \\ 
  score_{MAP} (\theta : \mathscr{D}) =& \ log (P (\mathscr{D} | \theta)) + log (P(\theta)) \nonumber\\
  \tilde{\theta} =& \operatorname*{argmax}_{\theta} score_{MAP}(\theta : \mathscr{D})
  \end{align}
  #+end_export

  Where the last equation follows immediately from the properties of
  the logarithm function. And the second equation from the fact that
  the normalizing constant does not depend on the parameter of
  interest.

  Given the above last property it is possible to understand that the
  conclusions from the previous chapter about the EM algorithm apply.

  In particular it is possible to adjust the M-step of the EM
  algorithm in order to have a properly working EM algorithm
  maximizing the score map of [[ref:eq:bayes_map]]. This will be shown in
  the next sections. 

    #+Begin_export latex
\end{article}
  #+end_export  

** Bayesian Parameter Learning - EM Generalization

   Maximum a posteriori Bayesian Parameter Learning is a
   straightforward generalization of the discussion of [[ref:math_em]].

   In fact noting that the score of the MAP estimator is defined as

   #+begin_export latex
   \begin{equation} 
   score_{MAP} (\theta : \mathscr{D}) =& \ log (P (\mathscr{D} | \theta)) + log (P(\theta)) 
   \end{equation}
   #+end_export

   it is possible to see that the previous results apply.

   In order to see that define the following adjusted energy
   functional:
   
   #+begin_export latex
   \begin{equation} \label{eq:adj_energy_functional}
   \tilde{F}[P, Q] = E_Q[log (\tilde{P})] + H_Q (X) + log (P(\theta)) 
   \end{equation}
   #+end_export

   Such that:

   #+begin_export latex
   \begin{align} \label{eq:adj_likelihood_energy_functional_relation}
   l (\theta: \mathscr{D}) + log (P(\theta)) =& \ \tilde{F}_D[\theta, Q] + D (Q (\mathscr{H}) || P (\mathscr{H}| \theta, \mathscr{D})) 
   \end{align}
   #+end_export  

   It follows immediately that maximizing the adjusted energy
   functional we are in fact maximizing the score-map such that the
   results of the previous section apply.

   The only question remaining is on how to optimize the adjusted
   energy functional via coordinate ascent optimization.

   Here it is straightforward to see that the E-step does not affect
   the adjusted metric but the M-step needs to be reformulated taking
   the effect of the prior into account.

   In order to see this consider our discussion in the previous
   chapter. The way you choose the Q distribution is unaffected and we
   will need to perform the same step in order to get the
   $\operatorname*{argmax}_{Q} \tilde{F}_D[\theta, Q]$.

   However, what is affected is the optimization along the other
   coordinate. That is the computation of
   $\operatorname*{argmax}_{\theta} \tilde{F}_D[\theta, Q]$ keeping Q
   fixed. In this case the terms depending on \theta is not limited to
   the expected likelihood E_Q[l (\theta: \mathscr{D}, \mathscr{H})]
   as was the case before but it is rather important to also consider
   the prior distribution $P(\theta)$.

   
   
**  Bayesian Parameter Learning - An Example

   An example for the extension of the EM algorithm to compute the
   maximum a posteriori parameter in the case of missing evidence is
   treated in this section.

   The theory proceeds with the most classic network structure. The
   one of table conditional probability distributions where the
   realizations are distributed according to a multinomial
   distribution given the \theta_{X_i | Pa_{X_i}} local parameters.

   Specifying a Dirichlet distribution as the prior of such parameters
   we can compute the maximum a posteriori estimator.

   As from the reasoning of the previous chapter we know that the EM
   algorithm properties of convergence and correctness apply and that
   the algorithm will iteratively converge to a local maximum.

   While as mentioned the E-step will be unaffected by the
   introduction of the prior, we need to adapt the M-step to account
   for the influence of the latter.

   Consider in this sense the score map for such dirichlet-multinomial
   posterior.

   #+begin_example
   WRITE DOWN THE EXPRESSION FOR THE SCORE MAP AND WRITE DOWN THE MAXIMUM OF IT.   
   #+end_example
   

   #+begin_export latex
   \begin{align} \label{eq:adj_likelihood_energy_functional_relation}
   l (\theta: \mathscr{D}) + log (P(\theta)) =& \ \tilde{F}_D[\theta, Q] + D (Q (\mathscr{H}) || P (\mathscr{H}| \theta, \mathscr{D})) 
   \end{align}
   #+end_export  

   - office managers

   
   

   


   -> have to write down the E and M step in terms of coordinate
   ascent optimization. would need it here then.

   

    
 \newpage

 bibliography:~/Desktop/Bayesian_Net_Thesis/literature/references.bib
 bibliographystyle:unsrt
  

** TODOs
   
*** TODO check if particle formulation in energy functional ok as such
    
*** TODO make more explicit the citation to koller and friedman in the chapter about the mathematics of the EM algo

