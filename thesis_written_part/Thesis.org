#+LATEX_CLASS: article
#+LATEX_HEADER: \usepackage{arxiv}
#+OPTIONS: toc:nil

#+begin_export latex
\newtheorem{theorem}{Theorem}

\title{Parameter Learning in Bayesian Networks under Uncertain Evidence  \textendash  \ An Exploratory Research.}
\author{
  Marco Hassan 	           	\\
  Zurich, CH		\\
  \\
  \\
  Master Thesis \\
  Presented to the Eidgenossische Teschnische Hochschule Zurich \\
  In Fulfillment Of the Requirements for \\ 
  the Master of Science in Statistics \\
  \\
  Supervisor: PhD. Radu Marinescu \\
  Co-Supervisor: Dr. Markus Kalisch \\
  %% examples of more authors
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\   
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{article}

\maketitle
#+end_export

\newpage

\tableofcontents

\newpage

* TODO Bayesian Networks Overview and Definition & Literature Review
  
* TODO Learning under Complete Evidence

* TODO Types of Uncertain Evidence
  
* TODO on the nastiness of the Likelihood function

  all good properties are gone when working with partially observed
  data. write down why here. 


* The Mathematics of the EM
  :PROPERTIES:
  :CUSTOM_ID: math_em
  :END:

  
  As discussed by cite:koller2009probabilistic it is possible to frame
  the EM as a coordinate ascent optimization of an energy function we
  will define next. Given such perspective we will be able to prove the
  following theorem

  #+begin_export latex
  \begin{theorem}\label{thm:one}
  Write here formally that the likelihood improves at each iteration step
  \end{theorem}
  #+end_export

  Consider the following energy function:

  #+begin_export latex
  \begin{equation} \label{eq:energy_functional}
  F[P(X), Q] = E_Q[log (\tilde{P}(X))] + H_Q (X)
  \end{equation}
  #+end_export

  Where $\tilde{P}$ is an unnormalized state probability $P =
  \frac{\tilde{P}}{Z}$ and $H_Q$ is the entropy of the observed
  particles. 

  Using such energy functional [[ref:eq:energy_functional]] it is possible
  to re-express the logarithm of the normalizing constant $Z$ as
  follows:

  #+begin_export latex
  \begin{equation} \label{eq:energy_refurmolation}
  log (Z) = F[P, Q] + D (Q||P)
  \end{equation}  
  #+end_export

  where $D(Q||P)$ is the Kullbackâ€“Leibler divergence, or relative
  entropy.

  We will choose next the following distribution for the particle
  distribution:

  #+begin_export latex
  \begin{equation} \label{eq:particle_distribution}
  P (H | D, \theta) =   \frac{P (H, D| \theta)}{P (D| \theta)}
  \end{equation}
  #+end_export

  With this choice it becomes clear that $Z (\theta) = P (D|
  \theta)$ and $\tilde{P} = P (H, Do| \theta)$. It
  follows then immediately that given such probability function we
  can compute the likelihood of realizations $\mathscr{D}, \mathscr{H}$:
  
  #+begin_export latex
  \begin{align} \label{eq:likelihood_particle}
  \mathscr{L} (\theta: \mathscr{D}, \mathscr{H}) =& \  P (\mathscr{H}, \mathscr{D}| \theta)\\
  \mathscr{L} (\theta: \mathscr{D}) =& \ P (\mathscr{D}| \theta)
  \end{align}
  #+end_export

  where $\mathscr{D}$ represents the observed evidence and
  $\mathscr{H}$ the missing evidence.

  Such that using [[ref:eq:energy_refurmolation]] we can get to the
  log-likelihood of the observed data in the following way:

  #+begin_export latex
  \begin{align} \label{eq:likelihood_energy_functional_relation}
  l (\theta: \mathscr{D}) =& \  F_D[\theta, Q] + D (Q (\mathscr{H}) || P (\mathscr{H}| \theta, \mathscr{D})) \\
  l (\theta: \mathscr{D}) =& \  E_Q[l (\theta: \mathscr{D}, \mathscr{H})]+ H_Q (\mathscr {H}) + D (Q (\mathscr{H}) || P (\mathscr{H}| \theta, \mathscr{D}))
  \end{align}
  #+end_export  

  The above are two fundamental equations. It is in fact
  straightforward to see that as both the relative entropy as well as
  the entropy are non-negative the log-likelihood on the left hand
  side above is an upper bound for the energy functional and the expected
  log-likelihood relative to Q, for any choice of Q.

  Moreover it is straightforward to see in the above that choosing the
  Q-measure as $P (H| D, \theta)$ the relative term
  fades away such that the entropy term is the overall measure on the
  difference between the expected log-likelihood and the real
  log-likelihood. It is in fact clear that in such a case the
  log-likelihood and the energy functional are the one and the same
  thing.

  In this sense the relation between the energy functional and the
  log-likelihood is clear and we can think of the EM-algorithm as a
  coordinate ascent optimization of the energy functional. To see this
  consider the E-step and M-step as follows.

** The Expectation Step

   Consider the first coordinate ascent - Q, keeping $\theta$
   fixed. We look for $\operatorname*{argmax}_{Q} F_D[\theta, Q]$. It
   is then immediate that:

   #+begin_export latex
   \begin{align} \label{eq:q_optimum}
   Q^* =& \ P (\mathscr{H}|\mathscr{D}, \theta) \\
   F_D[\theta, Q^*] =& \ l (\theta: \mathscr{D}) \\
   F_D[\theta, Q^*] \geq& \ F_D[\theta, Q]
   \end{align}
   #+end_export   

   The reasoning on why the above is the actual searched maximum
   argument is the following: You have in general an upper bound on the
   energy functional given by log-likelihood. If you now choose the
   distribution Q in the way described above you know that you have
   reached the upper bound and that such upper bound is tight. I.e. it
   is straightforward to see that your are at the maximum for a given
   \theta.

   Note that choosing $Q^*$ you are in fact choosing the probability
   density by which you are going to weight the synthetically created
   complete data sets in your E-step, so that you can in fact
   interpret the E-step as the step involving the maximization of the
   energy functional along the Q coordinate.

** The Maximization Step

    This is the second coordinate ascent - \theta. Here we look
    towards $\operatorname*{argmax}_{\theta} F_D[\theta, Q]$.

    It follows then the following quoting from
    cite:koller2009probabilistic:

    "Suppose Q is fixed, because the only term in F that involves \theta is
    $E_Q[l (\theta: \mathscr{D}, \mathscr{H})]$, the maximization is
    equivalent to maximizing the expected log-likelihood."

    It follows now that given the linearity of expectation it is
    possible to take the expectation of the sufficient statistics and
    maximizing for \theta.

    This is in fact exactly the standard M-step of the EM algorithm so
    that we can interpret the M-step as the coordinate ascent along
    the second axis. 
    
   Summarizing, by the fact that at each step the energy functional is
   optimized such that it increases it follows from proposition
   [[ref:eq:likelihood_energy_functional_relation]] that the
   log-likelihood increases such that theorem [[ref:thm:one]] is proved.


* Bayesian Parameter Learning

  A natural question that arises is whether it is possible to
  generalize the extended algorithm proposed by cite:Mrad_2015 to the
  case of Bayesian Parameter Learning.

  Recall that in Bayesian statistics rather than treating the
  parameters of interest as fixed but unknown you treat them as random
  variables themselves.

  You would then specify a prior, i.e. a probability distribution, for
  the data governing process of the parameters. This can be either a
  non-informative prior or a prior based on your domain knowledge
  expertise.

  Such prior distribution would then be updated upon the arrival of
  new observations according to the well known Bayes Rule. The result
  is an updated posterior distribution from which you can compute your
  statistics of interest.


  #+begin_export latex
  \begin{equation} \label{eq:bayes_formula}
  P (\theta | \mathscr{D}) = \frac{P (\mathscr{D} | \theta) * P(\theta)}{P (\mathscr{D})} 
  \end{equation}
  #+end_export

  It is straightforward to see that that the posterior is proportional
  to a likelihood term $P (\mathscr{D} | \theta)$ multiplied by the
  prior distribution.

  It is clear then, that depending on how you want to leverage the
  information of your posterior you would require a different
  mathematical exercise. I.e. in case you want to use as your
  point estimate of choice the expected value you would need an
  integration exercise and similar reasonings can be done for the
  other metrics.

  Another way you can set your parameters is by choosing the most
  likely point estimate. This is the maximum a posteriori point
  estimate and is defined in mathematical terms as follows:

  #+begin_export latex
  \begin{align} 
  \tilde{\theta} =& \operatorname*{argmax}_{\theta} \frac{P (\mathscr{D} | \theta) * P(\theta)}{P (\mathscr{D})} \nonumber\\
  \tilde{\theta} =& \operatorname*{argmax}_{\theta} P (\mathscr{D} | \theta) * P(\theta)\\ \label{eq:bayes_map}
  \tilde{\theta} =& \operatorname*{argmax}_{\theta} log (P (\mathscr{D} | \theta)) + log (P(\theta)) \nonumber \\
  \nonumber \\ 
  score_{MAP} (\theta : \mathscr{D}) =& \ log (P (\mathscr{D} | \theta)) + log (P(\theta)) \nonumber\\
  \tilde{\theta} =& \operatorname*{argmax}_{\theta} score_{MAP}(\theta : \mathscr{D})
  \end{align}
  #+end_export

  Where the last equation in (12) follows immediately from the properties of
  the logarithm function. And the second equation in (12) from the fact that
  the normalizing constant does not depend on the parameter of
  interest.

  Given the above it is possible to understand that the conclusions
  from the previous chapter about the EM algorithm apply. The first
  term of $score_{MAP}$ is exactly the likelihood term of the previous
  section. The only difference will be in the prior term.

  We will show next that it is possible to adjust the M-step of the EM
  algorithm in order to have a properly working EM algorithm
  maximizing the score map of [[ref:eq:bayes_map]]. This will be the main
  exercise of the next section.

    #+Begin_export latex
\end{article}
  #+end_export  

** Bayesian Parameter Learning - EM Generalization

   Maximum a posteriori Bayesian Parameter Learning is a
   straightforward generalization of the discussion of [[ref:math_em]].

   In fact noting that the score of the MAP estimator is defined as

   #+begin_export latex
   \begin{equation} 
   score_{MAP} (\theta : \mathscr{D}) =& \ log (P (\mathscr{D} | \theta)) + log (P(\theta)) 
   \end{equation}
   #+end_export

   it is possible to see that the previous results apply.

   In order to see that define the following adjusted energy
   functional:
   
   #+begin_export latex
   \begin{equation} \label{eq:adj_energy_functional}
   \tilde{F}[\theta, Q] = E_Q[log (\tilde{P}(X))] + H_Q (X) + log (P(\theta)) 
   \end{equation}
   #+end_export

   Such that:

   #+begin_export latex
   \begin{align} \label{eq:adj_likelihood_energy_functional_relation}
   l (\theta: \mathscr{D}) + log (P(\theta)) =& \ \tilde{F}_D[\theta, Q] + D (Q (\mathscr{H}) || P (\mathscr{H}| \theta, \mathscr{D})) 
   \end{align}
   #+end_export  

   It follows immediately that choosing $Q$ as $P
   (\mathscr{H}|\mathscr{D}, \theta)$ and maximizing the adjusted
   energy functional we are in fact maximizing the score-map such that
   the results of the previous section apply.

   The only question remaining is on how to optimize the adjusted
   energy functional via coordinate ascent optimization.

   Here it is straightforward to see that the E-step does not affect
   the adjusted metric but the M-step needs to be reformulated taking
   the effect of the prior into account.

   In order to see this consider our discussion in the previous
   chapter. The way you choose the Q distribution is unaffected and we
   will need to perform the same step in order to get the
   $\operatorname*{argmax}_{Q} \tilde{F}_D[\theta, Q]$.

   However, what is affected is the optimization along the other
   coordinate. That is the computation of
   $\operatorname*{argmax}_{\theta} \tilde{F}_D[\theta, Q]$ keeping Q
   fixed. In this case the terms depending on \theta is not limited to
   the expected likelihood E_Q[l (\theta: \mathscr{D}, \mathscr{H})]
   as was the case before but it is rather important to also consider
   the prior distribution $P(\theta)$.
   
**  Bayesian Parameter Learning - An Example

   An example for the extension of the EM algorithm to compute the
   maximum a posteriori parameter in the case of missing evidence is
   treated in this section.

   The theory proceeds with the most classic network structure. The
   one of table conditional probability distributions where the
   realizations are distributed according to a multinomial
   distribution given the \theta_{X_i | Pa_{X_i}} local parameters and
   where possible realizations $Val(X_i) = \{0,1 \}$.

   Specifying a Dirichlet distribution as the prior of such parameters
   we can compute the maximum a posteriori estimator.

   As from the reasoning of the previous chapter we know that the EM
   algorithm properties of convergence and correctness apply and that
   the algorithm will iteratively converge to a local maximum.

   While as mentioned the E-step will be unaffected by the
   introduction of the prior, we need to adapt the M-step to account
   for the influence of the latter.

   Consider in this sense the unnormalized probability for the
   Dirichlet-Multinomial distribution:

   #+begin_export latex
   \begin{align} \label{eq:dirichlet-multinomial-score}
   P_\theta(X) = \frac{\Gamma(\sum_i x_i + 1)}{\prod_i \Gamma(x_i + 1)} \prod_i^K \theta_{x_i | Pa_i}^{x_i}  * \frac{1}{B(\alpha)} \prod_{i=1}^K \theta_{x_i | Pa_i}^{\alpha_i - 1}
   \end{align}
   #+end_export

   And consider the adjusted energy functional
   [[ref:eq:adj_energy_functional]] from which we can derive the new
   likelihood expression in the case of missing evidence by defining a
   new random variable $Y$ expressing complete data observations
   $<\mathscr{H} = h, \mathscr{D} = d>$:
   
   #+begin_export latex
   \begin{align} \label{eq:dirichlet-multinomial-likelihood}
   \tilde{F}[\theta, Q] =& \ E_Q[P_\theta(Y)] + H_Q (Y)
   \end{align}
   #+end_export

   Such that taking the argument maximizing the likelihood of the
   adjusted energy functional $\operatorname*{argmax}_{\theta}
   \tilde{F}[\theta, Q]$ we are left with the following

   #+begin_export latex
   \begin{align} \label{eq:first-order-condition}
   \tilde{\theta} =& \operatorname*{argmax}_{\theta} \sum_m E_Q[log(\frac{\Gamma(\sum_i y[m]_i + 1)}{\prod_i \Gamma(y[m]_i + 1)} \prod_i^K \theta_{y_i | Pa{y_i}}^{y[m]_i} * \frac{1}{B(\alpha)} \prod_{i=1}^K \theta_{y_i | Pa{y_i}}^{\alpha_i - 1})] + H_Q (y[m]) \\
   \nonumber\\   
   \tilde{\theta} =& \operatorname*{argmax}_{\theta} \sum_m E_Q[log(\prod_i^K \theta_{y_i | Pa{y_i}}^{y[m]_i} * \theta_{y_i | Pa{y_i}}^{\alpha_i - 1})]\\
   \nonumber\\   
   \tilde{\theta} =& \operatorname*{argmax}_{\theta} \sum_m E_Q[log(\prod_i^K \theta_{y_i | Pa{y_i}}^{y[m]_i + \alpha_i - 1})] 
   \end{align}
   #+end_export

   It follows given that by the linearity of the expectation and that
   $y[m] = \{0,1\}$, we can re-express the above as:
   
   #+begin_export latex
   \begin{align} \label{eq:solution1}
   \tilde{\theta} =& \operatorname*{argmax}_{\theta} \sum_i^K (\sum_m^M E_Q[M[y_i, Pa_{y_i}]] + \alpha_i - 1) * log(\theta_{y_i | Pa{y_i}})] 
   \end{align}
   #+end_export

   where it holds

   #+begin_export latex
   \begin{align} \label{eq:expected_sufficient}
   \bar{M}[y_i, Pa_{y_i}]  =& \sum_m^M E_Q[M[y_i, Pa_{y_i}]]\\
   \bar{M}[y_i, Pa_{y_i}]  =& \sum_m^M \sum_{h[m] \in Val(\mathscr{H}[m])} Q(h[m]) \mathbbm{1}_{\{Y[m]_i = y[m]_i\}}\\
   \bar{M}[y_i, Pa_{y_i}]  =& \sum_m^M P(y_i | \mathscr{D}[m] = d[m], \theta)
   \end{align}
   #+end_export   

   So that ultimately:
   
   #+begin_export latex
   \begin{align} \label{eq:solution2}
   \tilde{\theta} =& \operatorname*{argmax}_{\theta} \sum_i^K (\bar{M}[y_i, Pa_{y_i}] + \alpha_i - 1) * log(\theta_{y_i | Pa{y_i}})] 
   \end{align}
   #+end_export      

   Given the additional restriction that $\sum_i \theta_{y_i |
   Pa{y_i}} = 1$, we can obtain the necessary condition for finding
   the optimum by using the Lagrange method

   #+begin_export latex
   \begin{align} \label{eq:first-order1}
   \frac{\partial}{\partial \theta_{y_i | Pa{y_i}}} \sum_i^K (\bar{M}[y_i, Pa_{y_i}] + \alpha_i - 1) * log(\tilde{\theta}_{y_i | Pa{y_i}})] - \lambda (\sum_i \tilde{\theta}_{y_i | Pa{y_i}} - 1) \mathrel{\stackon[5pt]{$=$}{$\scriptstyle!$}} 0
   \end{align}
   \begin{align} \label{eq:first-order2}
   \lambda = \frac{\bar{M}[y_i, Pa_{y_i}] + \alpha_i - 1}{\tilde{\theta}_{y_i | Pa{y_i}}}
   \end{align}
   #+end_export

   And inserting this in the first order condition and solving for
   $\tilde{\theta}_{y_i | Pa{y_i}}$

   #+begin_export latex
   \begin{align} \label{eq:solution}
   \tilde{\theta}_{y_i | Pa{y_i}} =& \frac{\bar{M}[y_i, Pa_{y_i}] + \alpha_i - 1}{\sum_j \bar{M}[y_j, Pa_{y_j}] + \alpha_j - 1}
   \end{align}
   #+end_export         

   
    
 \newpage

 bibliography:~/Desktop/Bayesian_Net_Thesis/literature/references.bib
 bibliographystyle:unsrt
  

** TODOs
   
*** TODO check if particle formulation in energy functional ok as such
    
*** TODO make more explicit the citation to koller and friedman in the chapter about the mathematics of the EM algo

