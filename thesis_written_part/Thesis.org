#+LATEX_CLASS: article
#+LATEX_HEADER: \usepackage{arxiv}
#+OPTIONS: toc:nil

#+begin_export latex
\newtheorem{theorem}{Theorem}

\title{Parameter Learning in Bayesian Networks under Uncertain Evidence  \textendash  \ An Exploratory Research.}
\author{
  Marco Hassan 	           	\\
  Zurich, CH		\\
  \\
  \\
  Master Thesis \\
  Presented to the Eidgenossische Teschnische Hochschule Zurich \\
  In Fulfillment Of the Requirements for \\ 
  the Master of Science in Statistics \\
  \\
  Supervisor: PhD. Radu Marinescu \\
  Co-Supervisor: Dr. Markus Kalisch \\
  %% examples of more authors
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\   
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{article}

\maketitle
#+end_export

\newpage

\tableofcontents
\listoffigures
\listofalgorithms
\listoftables

\newpage

* Bayesian Networks - Overview and Manuscript Outline


   Probabilistic models based on directed acyclic graphs (DAGs) have
   been consistently studied and researched since the first appearance
   in the field of genetics in cite:wright1921correlation.

   An important moment in the history of such models and the Bayesian
   Networks subclass was, as argued by cite:pearl2011bayesian, during
   the late 1970s when such models emerged as the method of choice when
   dealing with uncertainty in reasoning and expert systems;
   effectively starting to replace the usage of symbolic artificial
   intelligence and rule-based schema.

   As argued in cite:pearl2011bayesian, an especially significant
   property of such models that marked a turn-point in the modeling of
   real world systems was the fact that such models shifted the focus
   of the modeling exercise from reasoning processes focusing on the
   flow of information (think at classical AI systems including neural
   networks) to direct world representation, where edges may represent
   real-world causal connections.

   This, together with the possibility to associate probabilistic
   statements to events modeled by such graphs via bidirectional
   inference possibilities, makes the point for the usage and research
   of such models in various subfields of artificial intelligence.

   Given such properties and the fact that the mathematical machinery
   necessary to deal with such graphs spans multiple distinct
   subfields[fn:2]; the models attracted the interest of multiple
   researchers; it comes as no surprise the fact that there is a
   vibrant research community dealing with such topic. Alone in the
   last 10 (20) years the numbers of published papers on the topic
   increased by x4 (x24) factor[fn:1], with a total of ~100'000
   published papers on the topic of Bayesian Networks alone for the
   year 2020.

   In this sense, a thorough review of the ongoing research in the
   field would be impossible. While important progress has been done
   in researching the computational aspects of inference, learning and
   representation of the networks, most of the research so far is
   focusing on an idealized world with complete or missing random
   variables evidence. Hence, despite the wealth of research,
   fundamental issues when dealing with uncertain evidence for
   modeling Bayesian Networks have been only marginally addressed that
   far.

   This will pose the focus to this manuscript. By a rigorous literature
   review as well as some theoretical contributions we will try to
   partially address such a gap in the Bayesian Network modeling
   research. In this sense, we will start with a formal definition of
   Bayesian Networks in the next sub-section before turning to a
   rigorous definition of the different types of uncertain evidence
   from which we will expand on.

** Bayesian Networks - Formal Definition
   :LOGBOOK:
   CLOCK: [2021-05-21 Fri 16:22]--[2021-05-21 Fri 16:47] =>  0:25
   CLOCK: [2021-05-21 Fri 15:48]--[2021-05-21 Fri 16:13] =>  0:25
   CLOCK: [2021-05-21 Fri 14:18]--[2021-05-21 Fri 14:43] =>  0:25
   :END:

   Before starting to dig into the material we provide a general
   definition of a Bayesian Network as in cite:pearl2011bayesian.

   #+begin_export latex
  \begin{definition}
  Bayesian Network: A Bayesian Network is a directed acyclic graph (DAG) $G(\mathscr{V}, \mathscr{X})$
  whose nodes $\mathscr{X}$ represent random variables in the Bayesian sense - i.e. they can be observable
  quantities, latent variables, unknown parameters or hypotheses. \\
  On the top of it, edges $\mathscr{V}$ represent the conditional dependencies among the nodes; i.e. nodes that
  are not connected represent random variables that are conditionally independent.
  \end{definition}
   #+end_export

   Especially important is the characterization of the conditional
   independence relation among the random variables.

   In order to see this, recall that when modeling a Bayesian Network
   the object of interest is the complete probabilistic model of the
   random variables domain - i.e. the probability of every possible
   event as defined by the values of all of the variables.

   Consider the case where you would want to represent the joint
   distribution of a set of random variables $\mathscr{X} = \{X_1, ...,
   X_n\}$.

   Then, given a parametric distribution on each random variable in
   the set it is possible to extract the functional form of the
   probability distribution for the joint distribution. However, there
   is an issue when trying to directly model the joint distribution
   without explicitly leveraging and modeling the conditional
   independence structures among the random variables. This due to the
   exponential number of possible events in the domain that would
   require an exponential number of parameters. Standard examples may
   be found in cite:koller2009probabilistic.

   Yet when considering the conditional structure among the variables
   in the set $\mathscr{X}$, it is possible to leverage the /global
   semantics/ of the network to represent the joint-distribution in
   compact form cite:pearl2011bayesian:

   #+begin_export latex
   \begin{align*} 
   P (x_1, ..., x_n) = \prod_i P(x_i | pa_i)
   \end{align*}
   #+end_export      

   where $pa_i$ represents the parents nodes of node $x_i$.

   It is then possible to see that as well framed by
   cite:pearl2011bayesian: "Provided that the number of parents of
   each node is bounded, it is then immediate to see that the number
   of parameters required grows only linearly with the size of the
   network, whereas the joint-distribution itself grows
   exponentially."

   This leads us to a second possible definition of Bayesian network
   as framed by cite:koller2009probabilistic, making more concrete the
   idea of a Bayesian Network as a compact data structure representing
   the complete probabilistic model of the random variables domain:

   #+begin_export latex
  \begin{definition}
  Joint-Density Factorization: Let $\mathscr{G}$ be a Bayesian Network graph over the variables X_1, ..., X_n.
  
  We say that a distribution P over the same factorization space factorizes
  according to $\mathscr{G}$ if P can be expressed as a product 

  $$P (x_1, ..., x_n) = \prod_i P(x_i | pa_i^{\mathscr{G}})$$

  This equation is called the chain rule for Bayesian networks, and the $P(x_i | pa_i^{\mathscr{G}})$ terms are
  called conditional probability distributions (CPDs).
  \end{definition}


  \begin{definition}  
  Bayesian Network: A Bayesian Network is a pair $\mathscr{B} = (\mathscr{G}, P)$ where P factorizes over $\mathscr{G}$,
  and where P is specified as a set of CPDs associated with  $\mathscr{G}$â€™s nodes. The distribution P is often annotated P_{\mathscr{B}}.
  \end{definition}
   #+end_export

   Given such a definition it is immediate to distinguish the three
   main puzzles that is necessary to solve when working with Bayesian
   Networks, respectively:

   #+begin_quote
   (i) the structure riddle, where you specify either by domain knowledge
   or via data-driven learning the shape of the Bayesian Network graph
   $\mathscr{G}$. Both are challenging tasks especially in the case
   the user aims to fulfill both completeness and soundness of I-maps
   as defined in cite:koller2009probabilistic

   (ii) the parameterization learning riddle, where given some
   evidence we will try to specify the parameters of the CPDs of the
   network in the most sensible way such that we well describe the
   probabilistic model of the random variables domain.

   (iii) the inference riddle, where given the network structure and
   parameterization you would apply Bayes Theorem for performing
   bidirectional inference to get to the probabilistic occurrence of
   some random variables evidence. Again a very demanding task, given
   that it was proven that the exact solution of such problem is NP-hard,
   such that it is often necessary to rely on approximate inference
   methods as in cite:pearl1987evidential or variational methods as in
   cite:jordan1999introduction.
   #+end_quote


   In this manuscript, we will focus on (ii) taking (i) as given and
   leveraging standard results from the literature as in
   cite:koller2009probabilistic for (iii).

   We turn next to a formal definition of evidence and especially of
   /uncertain evidence/ necessary for the correct outline of the
   research presented in this manuscript.
    
** Types of Evidence

   The most basic case of evidence is the one of complete
   evidence. This occurs when we are provided with complete
   observations of the network, i.e.  when it is possible to observe a
   certain realization for each random variable in the domain of the
   network.

   One more interesting case is the one treated by cite:Mrad_2015,
   cite:Wasserkrug_all. The argument posed by the authors is that under
   many settings complete evidence is not possible.

   In many cases there might be a hiding mechanism active that might
   hide some of the realizations. Think for instance at a
   malfunctioning sensor that sporadically measures input. Or think for
   instance at medical settings where different patients might be
   measured different variables.

   Albeit the case of missing evidence greatly alters the way through
   which it is possible to learn the parameters of the network, there
   are multiple possible solutions to estimate parameters and come to
   local maxima. We will address one of such methods in the next
   chapter.

   A more interesting case is posed by /uncertain evidence/ as
   introduced by cite:Mrad_2015. The authors distinguish three types of
   non-complete evidence:

   (i) likelihood evidence

   (ii) fixed probabilistic evidence

   (iii) non-fixed probabilistic evidence

   We will use throughout this document the definition as in
   cite:Mrad_2015 which we will briefly summarize next.

   #+begin_export latex
   \begin{definition}
   Hard evidence: A finding on a variable commonly refers to an
   instantiation of the variable. This can be represented by a vector
   with one element equal to 1, corresponding to the state the variable
   is in, and all other elements equal to zero. This type of evidence
   is usually referred to as hard evidence.
   \end{definition}

   \\\\

   \begin{definition}
   Uncertain evidence: evidence that cannot be represented by a vector
   as in the hard evidence case.
   \end{definition}

   \\\\

   \begin{definition}
   Likelihood evidence: in such type of evidence there is uncertainty
   about the veracity of an observation, such as, for example, the
   information given by an imperfect sensor. Such uncertainty is
   expressed in terms of relative likelihood of observing one
   realization vis Ã  vis another one. 
   \end{definition}

   \\\\

   \begin{definition}
   Probabilistic evidence: we talk about probabilistic evidence when we
   have a set of probabilistic finding on one or multiple random variables X in the network
   specified by a local probability distribution R(X_i).
   \end{definition}  
   #+end_export

   Notice that a probabilistic finding $R(X_i)$ on a variable X_i of a
   Bayesian network replaces any prior belief or knowledge on X_i. As
   a consequence, the prior $P (X_i)$ resulting from Bayesian Network
   inference is not used in the propagation of $R(X_i)$, and any
   previous finding or belief on X_i is lost.

   Notice moreover the following distinction between /fixed/ and
   /non-fixed/ probabilistic evidence:

   #+begin_export latex
   \begin{definition}
   Fixed (Non-fixed) Probabilistic evidence: A probabilistic finding
   is fixed (non-fixed) when the distribution $R(X_i)$ can not be (can
   be) modified by the propagation of other findings.
   \end{definition}  
   #+end_export

   Such that it is all about how the /arrival of evidence/, as depicted
   in the following schema from cite:Mrad_2015, can update the
   cognitive state:

   #+CAPTION: Inference Loop as in Mrad et all.
   #+attr_latex: :width 5.0in  
   [[file:~/Desktop/Bayesian_Net_Thesis/images/inference_loop.png]]


   Summarizing, in simple terms, we differentiate the following three
   cases for the above:

   1. In fixed-evidence we specify a probabilistic evidence /all things
      considered/. This means that even after new evidence is observed
      on any other random variable in the network, we do not update the
      cognitive state specified by the fixed probabilistic evidence.
     
   2. In non-fixed probabilistic evidence we consider the current
      structure of the tree such that for the current state of the
      network, the conditional probability distribution is specified by
      the specified probabilistic evidence. Further in-coming evidence
      that will alter the network probabilistic structure will affect
      the cognitive state of the current node.

   3. In likelihood evidence we do not consider any prior
      information. I.e. we simply specify a local likelihood ratio for
      a particular evidence and we still have to run the inference step
      for the current state to get the final cognitive state. I.e. as
      mentioned by cite:Mrad_2015 in contrast to probabilistic evidence
      which remains unchanged by updating the observed variables,
      likelihood evidence has to be combined with previous beliefs in
      order to update the belief in the observed variable(s).

   Given the general understanding on Bayesian Networks and the
   different types of uncertain evidence we will provide in the next
   section a literature review for the case of parameter learning in
   Bayesian Networks in the case of complete, missing and uncertain
   evidence. We will finally propose the research outline for this
   manuscript. 

** Literature Review and Research Outline
  :PROPERTIES:
  :CUSTOM_ID: literature_review
  :END:

   Irrespective of the type of uncertainty, we distinguish among two
   class of methods in order to estimate parameters in Bayesian
   Networks, namely the Maximum Likelihood Estimation (MLE)
   cite:spiegelhalter1990sequential and Bayesian Learning
   cite:Smith_2001. The theoretical framework on which the two models
   build on is different. While in MLE we assume the parameter of
   interest to be a fixed but unknown variable, in Bayesian Learning
   we treat the parameter of interest themselves as a random
   variable.

   Both such methods have been extensively studied in statistical
   research.

   For MLE we have to guarantee the existence of such an estimator and
   then perform the optimization exercise in order to find the maximum
   likelihood estimator. In this sense a particular important result
   that we will leverage in the manuscript is the realization that in
   the case of an exponential family as treated by
   cite:barndorff1978hyperbolic, we can obtain the MLE as solving for
   the reverse I-projection (or M-projection) as defined in
   cite:csiszar1975divergence.

   In contrast to that in the case of Bayesian Learning the situation
   is more complex. On the one hand it is necessary to define a prior
   probability distribution on the parameter of interest before the
   flow of information arising from the observed evidence. This can be
   done either with the help of subject matter experts by expressing
   some degree of knowledge gained over the years, or by the usage of
   non-informative priors as in cite:syversveen1998noninformative. On
   the other hand, it is necessary to decide on a fix point estimator
   given a posterior distribution. Standard point estimator of choice
   are the maximum a posteriori estimator or the first moment of the
   posterior distribution. Important is in this sense to realize that
   we might not be able to integrate over every posterior such that it
   will be necessary to rely on approximate methods for computing the
   moments of interest. Or a possible different solution, is to
   restrict the modeling approach to exponential families with the
   respective conjugate priors, as defined in
   cite:schlaifer1961applied, such that the posterior will be a
   standard distribution such that moments and mode can be expressed
   by functions and coded efficiently in statistical software.

   Given the general theoretical framework for the two parameter
   estimation techniques mentioned above, we are left with a
   generalization of the above to the density function of Bayesian
   Networks. Seminal was the work of cite:spiegelhalter1990sequential
   who laid the foundation of the learning problem by expressing the
   assumption of global parameter independence coming to the central
   notion of decomposability of the likelihood function that we are
   going to treat in the next sections. Based on such work multiple
   papers were published expanding the theory for different classes of
   Bayesian Networks such as tree-CPDs cite:buntine1993tree, linear
   Gaussian BN cite:heckerman1995learning as well as different
   learning settings such as learning with parameter sharing networks,
   hierarchical Bayesian models and non-parametric estimation.

   The theoretical foundation for the theory exposed above was the one
   of complete evidence. Leaving such an ideal case and turning to
   missing evidence where missing values occurs for some network
   random variables, some further reasoning is
   necessary. cite:rubin1976inference and cite:little1976inference
   started to reason about the notion of /missing completely at
   random/ and /missing at random/ data, a topic we will re-encounter
   in later section of this manuscript. Given such a distinction it is
   possible to set out the fundamentals for the parameter learning
   task. We will show that in the case of /missing completely at
   random/ data some of the decomposability properties of the
   likelihood function under complete data are lost such that it is
   necessary to rely on some tailored learning technique such as the
   EM-algorithm first presented by cite:dempster1977maximum and its
   application to graphical models as in cite:lauritzen1995algorithm.
   We will show then that under missing evidence such an algorithm
   will be correct and converge to a local maxima of the joint
   distribution of the Bayesian Network.  

   Moving to the case of uncertain evidence, we will first build on
   the theory of cite:Wasserkrug_all to augment the EM-algorithm in
   order to deal with /likelihood evidence/ keeping the correctness
   and convergence properties of the algorithm. This will be done by
   exploiting the idea of augmenting Bayesian Network via virtual
   evidence nodes as outlined by cite:pearl2014probabilistic.

   Then in a second step we will generalize the theory of
   cite:Wasserkrug_all in order to the Bayesian Learning setting and
   the fixed point maximum a posteriori estimator. We will show there
   that it is possible to apply the EM algorithm in such a setting
   keeping in a way that its correctness and convergence properties
   are guaranteed.

   Finally, we will move to the case of /probabilistic
   evidence/. There we will reason how the algorithm proposed by
   cite:Wasserkrug_all will generalize to the case of /non-fixed
   probabilistic evidence/ in the case a single /probabilistic
   finding/ is expressed on a node of the Bayesian Network. We will
   then conclude the section by reasoning on how the iterative
   proportional fitting procedure might generalize EM algorithm to the
   case of an arbitrary number of probabilistic evidence in the
   network.

   Summing up, the manuscript continues as follows:

   - MAKE ROADMAP AND LINK CHAPTERS
        

* Learning under Complete Evidence
  :PROPERTIES:
  :CUSTOM_ID: complete-learning
  :END:

  In this section we will lay out the fundamental characteristics of
  parameter learning under complete evidence. On the one hand we will
  propose the global decomposition property as in
  cite:spiegelhalter1990sequential. On the other hand we will
  introduce the parameter independence condition; this is a
  fundamental property that Bayesian Networks need to display in order
  to fulfill the decomposition properties. In fact, we will see in the
  next section that in the case of missing evidence such a property is
  not fulfilled such that decomposability of Bayesian Networks are
  lost.

  The major result of cite:spiegelhalter1990sequential, was the
  realization that in the case of complete data the likelihood of the
  Bayesian Network decomposes over the set of local likelihood
  functions of the individual nodes *X_i*. Hence despite the fact that
  the likelihood function of Bayesian Networks is a product of
  multiple chained CPDs, it is possible to estimate the parameters of
  each CPDs locally, via one of the two presented learning methods, in
  order to get an overall network parameterization fulfilling the
  functional requirements of the estimation technique.

  In order to see that, consider the following network $\mathscr{B} =
  (\mathscr{G}, P)$ with \theta parameterization and a set of complete
  evidence $\mathscr{D}$ consisting of sample instances $\xi[1], ...,
  \xi[M]$ for nodes $i = 1, ..., k$.

  We note here, that in this manuscript we will work under the
  assumption that the parameters in the network are disjoint. Meaning
  that there is no /global parameter sharing/ as well as /local
  parameter sharing/ in the network as described in
  cite:koller2009probabilistic.

  Given such network structure it is possible to express the network
  overall probability via chain rule as:

  #+begin_export latex
  \begin{equation} \label{eq:global_decomposition}
  P_{\theta} (x_1, ..., x_n) = \prod_i \prod_m P_{\theta_i}(x_i[m] | Pa_i^{\mathscr{G}}[m])  \nonumber
  \end{equation}
  #+end_export

  Noting now that it is possible to invert the order of multiplication
  we get the following global likelihood decomposition for the
  Bayesian Network decomposition. 
  
  #+begin_export latex
  \begin{align} \label{eq:global_decomposition}
  L(\theta : \mathscr{D})       &=  P_{\theta} (x_1[m], ..., x_k[m]) \nonumber \\ 
  P_{\theta} (x_1[m], ..., x_k[m]) &= \prod_m \prod_i  P_{\theta_i}(x_i[m] | Pa_i^{\mathscr{G}}[m])  \nonumber \\
  P_{\theta} (x_1[m], ..., x_k[m]) &= \prod_i [\prod_m  P_{\theta_i}(x_i[m] | Pa_i^{\mathscr{G}}[m])]  \nonumber \\
                                &= \prod_i L(\theta_i : X_i[m])  \nonumber    
  \end{align}
  #+end_export

  It is then immediate to see that when solving for the MLE
  parameterization of the above you might well as well get your
  parameters by locally solving for the MLE estimators of the local
  likelihoods $L(\theta_i : X_i[m])$.

  A similar reasoning holds for the case of Bayesian Learning. In such
  a case we require on the top of complete data the property of /a
  priori independent/ parameters defined as follows as in
  cite:koller2009probabilistic

    #+begin_export latex
   \begin{definition} \label{def:a_priori_parameters}
   A Priori Global Parameter Independece: Let G be a Bayesian network
   structure with parameters \theta = (\theta_{X_1|Pa_{X_1}} , ...,
   \theta_{X_n|Pa_{X_n}}).

   A prior $P(\theta)$ is said to satisfy global parameter independence
   if it has the form:

   $$P(\theta) = \prod_i P(\theta_{X_i}|Pa_{X_i})$$
   
   \end{definition}  
   #+end_export

   Such that with /a-priori global parameter independence/ you have
   the property that knowing the value of one parameter does not add
   any information regarding another parameter value. 

   With this notion in mind and the global likelihood decomposition
   property it is possible to see that it holds for Bayesian Learning
   under complete data:
   
   #+begin_export latex
   \begin{align} \label{eq:bayes_learning_complete_data}
   P_{\theta} (x_1[m], ..., x_k[m]) &= \prod_i L(\theta_i : X_i[m])  \nonumber \\
   P(\theta) &= \prod_i P(\theta_{X_i}|Pa_{X_i})   \nonumber \\
   P(\theta | \mathscr{D}) &= \frac{1}{P(\mathscr{D})} \prod_i L(\theta_i : X_i[m]) * P(\theta_{X_i}|Pa_{X_i}) \nonumber
   \end{align}
   #+end_export

   Such that once more we might for instance be able to compute the
   maximum a posteriori parameterization for the entire network, by
   taking the maximum a posteriori parameterization of individual
   CPDs.

   As a final note, we stress here the point that, in a similar line
   of reasoning, if observing the data the parameterization for a
   local CPD are d-separated - such that, after observing complete
   data, $\theta_{Y|pa_i_j}$ is independent from $\theta_{Y|pa_i_l}$
   for all local parents $j,l = 1, ..., p$ and with $i \neq l$ - then
   a network satisfies the local decomposability property. In such a
   case it would then hold for the parameter local independence:

   #+begin_export latex
   \begin{align}
   P(\theta) &= \prod_i P(\theta_{X_i}|Pa_i) \nonumber \\ 
             &= \prod_i \prod_j P(\theta_{X_i}|pa_i_j) \nonumber  
   \end{align}
   #+end_export

   And correspondingly for your likelihood
   
   #+begin_export latex
   \begin{align} 
   L(\theta : \mathscr{D}) &= \prod_i^K [\prod_m^M [\prod_j^P  P_{\theta_i_j}(x_i[m] | pa_i_j^{\mathscr{G}}[m])] \nonumber \\
                           &= \prod_i^K [\prod_j^P [\prod_m^M  P_{\theta_i_j}(x_i[m] | pa_i_j^{\mathscr{G}}[m])]] \nonumber \\
                           &= \prod_i \prod_j L(\theta_i_j : X_i[m])  \nonumber
   \end{align}
   #+end_export

   Such that it would ultimately hold for Bayesian Learning

   #+begin_export latex
   \begin{align} 
   P(\theta | \mathscr{D}) &= \frac{1}{P(\mathscr{D})} \prod_i \prod_j L(\theta_i_j : X_i[m]) * P(\theta_{X_i}|pa_i_j) \nonumber
   \end{align}
   #+end_export

   It is then clear that in such a case it is then possible to
   maximize at an even more narrow local level reducing the difficulty
   of the optimization task.
   
   We turn next to the case of missing evidence where we are going to
   see that such neat properties fade away such that it will be
   necessary to rely on more sophisticated techniques in order to
   perform the learning task. In fact, as we will see it will not be
   possible anymore to perform local operations for coming to a global
   solution.

* On Missing Evidence

  In the case of missing evidence we have two types of findings for
  the random variables in our network $G(\mathscr{V}, \mathscr{X})$.

  Once more, consider $m = 1, ..., M$ instances of your network. Then
  on the one hand you will have observed random variables realizations
  $d[m]$ for a subset of variables $\mathscr{D} \subset
  \mathscr{X}$. On the other hand you will have missing or
  non-observed findings $h[m]$ for a subset of variables
  $\mathscr{H} \vcentcolon= \mathscr{X} \textbackslash \mathscr{D}$.

  As both of the parameter learning techniques, as presented in
  [[ref:literature_review]], involve a likelihood term, the question is on
  the way such likelihood term can be represented in the case of
  missing evidence.

  In order to answer such a question we will shortly distinguish among
  data /missing completely at random/ and data /missing at random/ as
  reasoned in cite:little1976inference, cite:rubin1976inference.

  We start by defining a data hiding mechanism $P_\psi(O_{X_i}|X_i)$ where
  $O_{X_i}$ is a binary random variable representing whether the
  random variable $X_i$ is observed or missing. It then follows that
  it is possible to express the probability of the random variable
  $X_i$ realization through $P_{missing}(X_i, O_{X_i}) = P_{theta}(X_i) *
  P_\psi(O_{X_i}|X_i)$.

  Given such a model we turn to the definition of the two essential
  hiding mechanisms leveraging once more the work of cite:koller2009probabilistic:

  #+begin_export latex
  \begin{definition}
  Missing Completely at Random: A missing data model $P_{missing}$ governing a
  random variable $X_i$ is missing
  completely at random (MCAR) if $P_{missing} \models (X_i \perp O_{X_i})$.
  I.e. in the case marginal independence among the the observation mechanism
  and the random variable.
  \end{definition}  
  #+end_export

  Then given such property it is immediate to see that the likelihood
  decomposes on terms depending on the parameters of interest
  $\theta$ and on terms governing the data hiding mechanism
  $\psi$. If the ultimate interest of your study is on the
  parameterization of the data governing mechanism of the random
  variables $X_i$, i.e. on $\theta$, it is then obvious that can just
  focus on such portion of the likelihood function forgetting about
  the data hiding mechanism.

  Then defining the set of random variables $Y = \{Y_1, . . . , Y_n\}$,
  where $Val(Y_i) = Val(X_i) \cup \{?\}$, where $\{?\}$ represents a
  missing evidence, we can define the following data hiding mechanism:

  #+begin_export latex
  \begin{definition}
  Missing at Random: A data model $P_{missing}$ is missing at random (MAR)
  if for all observations with $P(y) > 0$, i.e. if for all possible realizations,
  and for all $h \in Val(\mathscr{H})$, we have with $d \in \mathscr{D}$
  observed evidence, that $ P_{missing} \models (h \perp o_{X_i} | d) $.
  \end{definition}  
  #+end_export

  Or in other words, we talk about data /missing at random/ when
  conditioning on the observed evidence we have conditional
  independence among the hidden, non-observed variables, and the
  hiding mechanism.

  As pointed out by cite:koller2009probabilistic, MAR is a powerful
  condition as it is a necessary condition in order to write the
  likelihood function under missing evidence as a product of terms
  involving the parameters governing the probabilistic structure of
  the random variables of interest $X_i$ and the hiding mechanism
  $O_{X_i}$. It is then possible as in the case of /missing completely
  at random/ to distinguish between the two likelihood terms and just
  focus on the likelihood of the observed variables when estimating
  the \theta parameters.

  In order to see this note first that in the case of MAR, the
  observation pattern $o_X$ gives no additional information about the
  hidden variables given the observed variables, that is:

  $$ P_{missing} (h | d, o_{X_i}) =  P_{missing} (h | d) $$

  It holds then that 
  
  
   #+begin_export latex
   \begin{align}
   P_{missing}(y) &= \sum_h P_{\theta} (h, d) * P_\psi(o_{X_i} | h, d) \nonumber \\
                &= \sum_h P_{\theta} (h, d) * P_\psi(o_{X_i} | d) \nonumber \\
                &= P_\psi(o_{X_i} | d) * \sum_h P_{\theta} (h, d)  \nonumber \\
		&= P_\psi(o_{X_i} | d) * P_{\theta} (d)  \nonumber		
   \end{align}
   #+end_export

   Such that you can easily see that if $P_{missing}$ is MAR then
   $L(\theta, \psi : \mathscr{D})$ decomposes into two terms $L(\theta :
   \mathscr{D}), L(\psi : \mathscr{D}, O_X)$.

   Noting now that as we can always reach the /MAR/ condition by
   expanding a Bayesian Network, we will assume for the theory in this
   manuscript that the Bayesian Network of interest presenting missing
   evidence satisfies /MAR/, such that the question of interest will
   be the functional form of the likelihood $P_{\theta} (d)$ in the
   case of missing data, the topic we will address in the next
   section.

** On the Observed Variables Likelihood under Missing Data

   This section sets the focus on the likelihood of the observed data
   in the case of missing evidence.

   We know in fact that in a Network with missing data satisfying
   /MAR/, it is possible to just focus on such a term forgetting the
   parameters governing the data hiding mechanism in order to estimate
   the parameterization \theta that maximizes the likelihood of the
   random variables of interest $X_i$.

   Starting from the principle it holds that for a set of observed
   variables $\mathscr{D}$ we have:

   $$ L(\theta: \mathscr{D}) = \prod_m^M P_\theta(d[m]) $$

   Despite the above looks similar to the case of complete data
   observations and we might be tempted to say that the learning task
   does not differ, the difference lies in the fact that under missing
   evidence we loose the parameter independence property. This because
   as we will reason next in the case of missing evidence, the trails
   among parameters in the networks are not anymore d-separated such
   that information on one node will not only yield information for
   the particular node parameters governing the observation but rather
   yield information for other local and global network parameters as well.

   In order to understand why the decomposition property is gone think
   for instance at the following basics network structure with
   table-CPDs and binary random variables: $\mathscr{G}_{X_1
   \rightarrow X_2}$. It follows then that you have six parameters
   governing the random variables realizations: $\theta_{x_1^0},
   \theta_{x_1^1}, \theta_{x_2^1| x_1^1}, \theta_{x_2^0 | x_1^1},
   \theta_{x_2^1 | x_1^0}, \theta_{x_2^0 | x_1^0}$.

   To see why the local decomposition is lost in the above graph
   consider the case:

   $$\theta_{X_2 | x_1^1} \rightarrow X_2 \leftarrow \theta_{X_2 |
   x_1^0}$$

   It is then straightforward to see that observing both $X_2$ and
   $X_1$, $\theta_{X_2 | x_1^0}$ and $\theta_{X_2 | x_1^1}$ are
   d-separated as we can rule out the arcs that are not
   active. However, when $X_1$ is missing with $X_2$ being observed
   the above will be d-connected due to the common effect factor. In
   this sense local decomposability is lost and we will have to /sum/
   up the likelihoods of both the case $x_1^1$ and $x_1^0$.

   An analogous case emerges for the case of the global
   decomposition. Think for instance at the network:

   $$ X_2 \leftarrow  H \rightarrow X_1 $$

   Then in the case of missing $H$; $X_1$ and $X_2$ would be
   d-connected due to the common factor and no-inactive arcs. It
   follows once more that the likelihood would be given by the sum of
   all of the possible likelihood realizations of the missing variable
   $H$, such that the likelihood would be given in general by the
   following expression:

   $$ L(\theta: \mathscr{D}) = \prod_m \sum_h P_\theta(d, h)$$

   It is immediate to see that it is not anymore possible to invert
   the order of the multiplication due to the interaction of summing
   and multiplication operations. Moreover, it is also immediate to
   see that the above will require an inference step to get to the
   probabilities of the observations.
   
   In this sense both the /local/ as well as the /global/ likelihood
   decomposition properties are possibly lost under missing evidence
   and the computational difficulty of the learning task increases, as
   it is necessary to deal with multimodal likelihood arising from the
   sum of unimodal distributions.

   We will cover in the next section the idea of the
   EM-algorithm. This emerged as a powerful algorithm in order to deal
   with the difficulties that arise from such a complex multinomial
   likelihood function. We will see that due to an expectation step we
   will restore an /expected/ likelihood decomposability
   property. Moreover, we will see by reviewing the EM-theory that
   convergence to local maxima is guaranteed such that we know that
   such a method will reach one of the local maxima of the likelihood
   distribution of the observed data.


** The Mathematics of the EM
   :PROPERTIES:
   :CUSTOM_ID: math_em
   :END:
  
   As discussed by cite:koller2009probabilistic it is possible to frame
   the EM as a coordinate ascent optimization of an energy function we
   will define next. Given such perspective we will be able to prove the
   following theorem

   #+begin_export latex
   \begin{theorem}\label{thm:one}
   Write here formally that the likelihood improves at each iteration step
   \end{theorem}
   #+end_export

   Consider the following energy function:

   #+begin_export latex
   \begin{equation} \label{eq:energy_functional}
   F[P(X), Q] = E_Q[log (\tilde{P}(X))] + H_Q (X)
   \end{equation}
   #+end_export

   Where $\tilde{P}$ is an unnormalized state probability $P =
   \frac{\tilde{P}}{Z}$ and $H_Q$ is the entropy of the observed
   particles. 

   Using such energy functional [[ref:eq:energy_functional]] it is possible
   to re-express the logarithm of the normalizing constant $Z$ as
   follows:

   #+begin_export latex
   \begin{equation} \label{eq:energy_refurmolation}
   log (Z) = F[P, Q] + D (Q||P)
   \end{equation}  
   #+end_export

   where $D(Q||P)$ is the Kullbackâ€“Leibler divergence, or relative
   entropy.

   We will choose next the following distribution for the particle
   distribution:

   #+begin_export latex
   \begin{equation} \label{eq:particle_distribution}
   P (\mathscr{H} | \mathscr{D}, \theta) =   \frac{P (\mathscr{H}, \mathscr{D}| \theta)}{P (\mathscr{D}| \theta)}
   \end{equation}
   #+end_export

   With this choice it becomes clear that $Z = P (\mathscr{D}|
   \theta)$ and $\tilde{P} = P (\mathscr{H}, \mathscr{D}| \theta)$. It
   follows then immediately that given such probability function we
   can compute the likelihood of realizations $\mathscr{D}, \mathscr{H}$:
  
   #+begin_export latex
   \begin{align} \label{eq:likelihood_particle}
   L (\theta: \mathscr{D}, \mathscr{H}) =& \  P (\mathscr{H}, \mathscr{D}| \theta)\\
   L (\theta: \mathscr{D}) =& \ P (\mathscr{D}| \theta)
   \end{align}
   #+end_export

   where $\mathscr{D}$ represents the observed evidence and
   $\mathscr{H}$ the missing evidence.

   Such that using [[ref:eq:energy_refurmolation]] we can get to the
   log-likelihood of the observed data in the following way:

   #+begin_export latex
   \begin{align} \label{eq:likelihood_energy_functional_relation}
   l (\theta: \mathscr{D}) =& \  F_D[\theta, Q] + D (Q (\mathscr{H}) || P (\mathscr{H}| \theta, \mathscr{D})) \\
   l (\theta: \mathscr{D}) =& \  E_Q[l (\theta: \mathscr{D}, \mathscr{H})]+ H_Q (\mathscr {H}) + D (Q (\mathscr{H}) || P (\mathscr{H}| \theta, \mathscr{D}))
   \end{align}
   #+end_export  

   The above are two fundamental equations. It is in fact
   straightforward to see that as both the relative entropy as well as
   the entropy are non-negative the log-likelihood on the left hand
   side above is an upper bound for the energy functional and the expected
   log-likelihood relative to Q, for any choice of Q.

   Moreover it is straightforward to see in the above that choosing the
   Q-measure as $P (\mathscr{H}| \mathscr{D}, \theta)$ the relative term
   fades away such that the entropy term is the overall measure on the
   difference between the expected log-likelihood and the real
   log-likelihood. It is in fact clear that in such a case the
   log-likelihood and the energy functional are the one and the same
   thing.

   In this sense the relation between the energy functional and the
   log-likelihood is clear and we can think of the EM-algorithm as a
   coordinate ascent optimization of the energy functional. To see this
   consider the E-step and M-step as follows.

*** The Expectation Step

    Consider the first coordinate ascent - Q, keeping $\theta$
    fixed. We look for $\operatorname*{argmax}_{Q} F_D[\theta, Q]$. It
    is then immediate that:

    #+begin_export latex
    \begin{align} \label{eq:q_optimum}
    Q^* =& \ P (\mathscr{H}|\mathscr{D}, \theta) \\
    F_D[\theta, Q^*] =& \ l (\theta: \mathscr{D}) \\
    F_D[\theta, Q^*] \geq& \ F_D[\theta, Q]
    \end{align}
    #+end_export   

    The reasoning on why the above is the actual searched maximum
    argument is the following: You have in general an upper bound on the
    energy functional given by log-likelihood. If you now choose the
    distribution Q in the way described above you know that you have
    reached the upper bound and that such upper bound is tight. I.e. it
    is straightforward to see that your are at the maximum for a given
    \theta.

    Note that choosing $Q^*$ you are in fact choosing the probability
    density by which you are going to weight the synthetically created
    complete data sets in your E-step, so that you can in fact
    interpret the E-step as the step involving the maximization of the
    energy functional along the Q coordinate.

*** The Maximization Step

     This is the second coordinate ascent - \theta. Here we look
     towards $\operatorname*{argmax}_{\theta} F_D[\theta, Q]$.

     It follows then the following quoting from
     cite:koller2009probabilistic:

     "Suppose Q is fixed, because the only term in F that involves \theta is
     $E_Q[l (\theta: \mathscr{D}, \mathscr{H})]$, the maximization is
     equivalent to maximizing the expected log-likelihood."

     This is in fact exactly the standard M-step of the EM algorithm so
     that we can interpret the M-step as the coordinate ascent along
     the second axis. 
    
    Summarizing, by the fact that at each step the energy functional is
    optimized such that it increases it follows from proposition
    [[ref:eq:likelihood_energy_functional_relation]] that the
    log-likelihood increases such that theorem [[ref:thm:one]] is proved.

** An Exponential Family Example

    This section provides an application of theory presented above for
    the general case of exponential families. The idea is to
    crystallize the theory developed so far in the general setting of
    exponential families CPDs.

    Given such a procedure it will be possible for the user to apply
    the presented theory to a general class of distribution allowing
    rich modeling for probabilistic graphical models.

    In order to see this define at first the set $\mathscr{Q}$ of
    parametric distributions belonging to the exponential family
    P_{\theta}(X), defined as:

    #+begin_export latex
    \begin{align} \label{eq:exponential-family}
    P_{\theta}(X) = \frac{1}{Z(\theta)} exp[\sum_i c(\theta_i)\tau(X_i)] * A(X)
    \end{align}
    #+end_export

    where, $Z(\theta)$ is a normalizing term and $\tau(X) = (\tau(X_1),
    ..., \tau(X_K))$ is the sufficient statistics.

    You can then see that multiple distributions belong to such class
    of distributions.

    Consider for instance the most basic case when modeling Bayesian
    Networks, the one of multinomial table-CPDs. You can then see that
    such distributions belong to the exponential family.

    Recall that for the multinomial table-CPDs with binary $X_i$ the
    local probability function is given by:

    #+begin_export latex
    \begin{align} \label{eq:multinomial-cpd}
    P(X_i|\theta) = \prod_{x_i \in Val(X_i), pa_i \in Val(Pa_i)} \theta_{x_i | pa_i}^{x_i}
    \end{align}
    #+end_export

    You can now frame the above in the exponential family form by
    defining the sufficient statistics as $\tau(x_i | pa_i) =
    \mathbbm{1}_{\{X = x, Pa_i = pa_i : x \in Val(X), pa_i \in
    Val(Pa_i)\}}$ and $c(\theta_{x_i | Pa_i}) = ln(\theta_{x_i |
    pa_i})$.

    Given that it is immediate to see that
    
    #+begin_export latex
    \begin{align} \label{eq:multinomial-cpd}
    P(X_i|\theta) = exp[\sum_{x_i \in Val(X_i), pa_i \in Val(Pa_i)} c(\theta_{x_i | pa_i}) * \tau(x_i | pa_i)] 
    \end{align}
    #+end_export

    Another of such examples are linear Gaussian Bayesian networks. In
    such networks the local probability model is defined follows, for
    a node defined by the random variable X_i it holds:

    #+begin_export latex
    \begin{align} \label{eq:local-prob-model}
    X_i = \beta_{i0} + \beta_{i1} * pa_{i1} + ... + \beta_{ip} * pa_{ip} + \epsilon
    \end{align}
    #+end_export    

    where $\epsilon \sim N(0,\sigma^2)$.

    Given such definition you have that:

    #+begin_export latex
    \begin{align} \label{eq:gaussian-cpd}
    P(X_i|\theta_i) = \frac{1}{\sqrt{2\pi\sigma_i^2}} exp[-\frac{1}{2\sigma_i^2} (x_i - (\beta_{i0} + \beta_{i1} * pa_{i1} + ... + \beta_{ip} * pa_{ip}))^2] 
    \end{align}
    #+end_export        

    You can then see by expanding the square that the sufficient
    statistics for such local exponential distribution is: $\tau(x|pa) =
    (1,x,pa_1, ..., pa_p, x^2, xpa_1, . . . , xpa_p, pa_1^2, pa_1pa_2,
    . . . , pa_p^2)$.

    Leaving such examples and going back to the general definition of
    exponential family distributions it is immediate to see that if
    the local CPDs are exponential family distributions, the global
    probability function over the entire network will be an
    exponential family distribution.

    Given such a local CPD it follows from the theory of the previous
    section that in the case of /complete data/, we can solve for the
    global MLE by locally maximizing individual CPDs. You can then get
    the MLE of the CPDs by either deriving the MLE by standard
    analytical theory or by means of M-projection theory and moment
    matching as argued by cite:koller2009probabilistic.

    This means that for a general exponential family you have a local
    CPD likelihood on M complete instance you can maximize:

    #+begin_export latex
    \begin{align} \label{eq:exponential-family-likelihood}
    P(X_i|\theta_i) = \prod_m^M \frac{1}{Z(\theta_i)} exp[c(\theta_i)^\intercal \tau(X_i[m])] * A(X_i[m]) 
    \end{align}
    #+end_export            

    Consider now the case of /missing evidence/. Here again it is
    possible to apply the theory exposed in the previous section in a
    straightforward way. In this sense we showed how alternating an
    M-step maximizing the likelihood of missing and observed values
    and an E-step performing some inference for a given
    parameterization, we are guaranteed to reach a local maxima for
    the likelihood of the observed data.

    In the case of missing evidence, for each instance we might have
    both observed evidence $d_i[m]$ as well as missing evidence $h_i[m]$.

    Given the inference step where, given the current network
    parameterization, we compute the probabilistic realization of
    possible synthetically complete data, we can express and maximize
    the following expected likelihood function.

    #+begin_export latex
    \begin{align} \label{eq:complete-exponential-family-likelihood}
    E_Q(l(\theta_i :D_i, H_i)) =& \ - Mlog(Z(\theta_i) + \sum_m^M \sum_{h_i[m] \in Val(\mathscr{H}_i[m])} Q(h_i[m]) * \mathbf{c(\theta_i)}^\intercal \mathbf{\tau}(d_i[m], h_i[m])\\
                & + \sum_m^M \sum_{h_i[m] \in Val(\mathscr{H}_i[m])} Q(h_i[m]) * log(A(d_i[m], h_i[m]))  \nonumber \\
    E_Q(l(\theta_i :D_i, H_i)) =& \ - Mlog(Z(\theta_i)) + \sum_m^M E_Q[\mathbf{c(\theta_i)}^\intercal \mathbf{\tau}(d_i[m], h_i[m])] + E_Q[log(A(d_i[m], h_i[m]))]
    \end{align}
    #+end_export                

    Note at last that given the above inference step you would have
    for the global likelihood with K factors

    #+begin_export latex
    \begin{align} \label{eq:global-likelihood}
    E_Q(l(\theta :D, H)) =& \ \prod_i^K E_Q(l(\theta_i :D_i, H_i)) \nonumber \\
                   =& \ \prod_i^K - Mlog(Z(\theta_i)) + \sum_m^M E_Q[\mathbf{c(\theta_i)}^\intercal \mathbf{\tau}(d_i[m], h_i[m])] + E_Q[log(A(d_i[m], h_i[m]))] \\
                   =& \ \prod_i^K - Mlog(Z(\theta_i)) + \mathbf{c(\theta_i)}^\intercal \sum_m^M E_Q[\mathbf{\tau}(d_i[m], h_i[m])] + E_Q[log(A(d_i[m], h_i[m]))] \nonumber  
    \end{align}
    #+end_export                

    Hence, it is possible to see that due to the linearity of the
    expectation we have global decomposability of the expected
    likelihood function such that we can estimate the global MLE of
    the expected likelihood of the network by estimating the local MLE
    of the CPDs expected likelihood.

    Performing this exercise for the two examples above we get the
    following.

    Starting with the multinomial table CPDs and defining a random
    variable $Y$ representing the /synthetically completed data/ $<H, D>$,
    we have that

    #+begin_export latex
    \begin{align} \label{eq:solution}
    \tilde{\theta}_{y_i | Pa_i} =& \operatorname*{argmax}_{\theta_{y_i | Pa_i}}  \prod_m \prod_{y_i \in Val(Y_i)} P(Y_i[m]|\theta_i) \nonumber  \\
    \tilde{\theta}_{y_i | Pa_i} =& \operatorname*{argmax}_{\theta_{y_i | Pa_i}} \sum_m \sum_{y_i \in Val(Y_i), pa_i \in Val(Pa_i)} ln(\theta_{y_i | pa_i}) * \sum_{h[m] \in Val(\mathscr{H}[m])} Q(h[m]) * \mathbbm{1}_{\{y_i = y_i[m], pa_i = pa_i[m]\}}
    \end{align}
    #+end_export
    
    With the additional constraints that $\sum_{y_i \in Val(Y_i), pa_i
    \in Val(Pa_i)} \theta_{y_i | pa_i} = 1$.

    Solving this constrained optimization problem by standard
    Lagrange method you get: 

    #+begin_export latex
    \begin{align} \label{eq:solution}
    \tilde{\theta}_{y_i | Pa_i} =& \frac{\bar{M}[y_i, Pa_i]}{\sum_j \bar{M}[y_j, Pa_j]}
    \end{align}
    #+end_export

    With $\bar{M}[y_i, Pa_i] = \sum_m^M \sum_{h[m] \in
    Val(\mathscr{H}[m])} Q(h[m]) * \mathbbm{1}_{\{y_i = y_i[m], pa_i =
    pa_i[m]\}} = E_Q(M(y, pa)), \ M(y, pa) = \sum_m \tau(y,pa)$.
    
    Algorithmically it is then possible to write such an EM-application for
    the above case as in [[ref:alg:EM-Likelihood-Complete data]]

   #+begin_export latex
\algrenewcommand\algorithmicindent{1.5em}%

\begin{algorithm*}[h!]
\caption{EM-Likelihood: an EM algorithm for learning with likelihood evidence}
\label{alg:EM-Likelihood-Missing-Data}
%\begin{\algsize}
\vspace{-10pt}
\begin{multicols}{2}
\begin{algorithmic}[1] 
\Require Bayesian network $\mathcal{B}=\langle \mathbf{X},\mathbf{D}, G, \mathbf{P} \rangle$, dataset $S$ 

\Procedure{EM}{$\mathcal{B}$, $S$}
\State Initialize $\mathcal{B}$'s parameters $\theta \leftarrow \theta^0$
\ForAll{$t=1, \ldots$ until convergence}

  \State $\left\{ \bar{M}_{\theta^t}[x_{i},u_{i}]\right\} \leftarrow$\textsc{Compute-ESS}($\mathcal{B}=(G,\theta^{t})$, $S$)

  \ForAll{$i=1, \ldots, n$}

    \ForAll{$x_{i},u_{i}\in Val(X_{i},Pa_{X_{i}}^{\mathcal{B}})$}

      \State $\theta_{x_{i}|u_{i}}^{t+1}=\frac{\bar{M}_{\theta^{t}}[x_{i},u_{i}]}{\bar{M}_{\theta^{t}}[u]}$
    \EndFor
  \EndFor
\EndFor
\EndProcedure
\\
\Function{Compute-ESS}{$\mathcal{B}=(G,\theta)$, $S$} 

\ForAll {$i\in1,\ldots,n$}
  \ForAll {$x_{i},u_{i}\in Val(X_{i},Pa_{X_{i}}^{\mathcal{B}})$}
   \State $\bar{M}[x_{i},u_{i}]\leftarrow 0$
  \EndFor
\EndFor

% \State (Go over all evidence nodes, creating an augmented network
% for each one, and collect all of the evidence for the nodes in $G$)
\ForAll{example $S_{j}\in S$}
    \State Run inference on $(G,\theta)$ with evidence $d_{j}$
    \ForAll{i$ = 1,\ldots,n$}
      \ForAll{$x_{i},u_{i}\in Val(X_{i},Pa_{X_{i}}^{\mathcal{B}})$}
    
        \State $\bar{M}[x_{i},u_{i}] \mathrel{{+}{=}} P_{(G,\theta)}(x_{i},u_{i}|d_{j})$
      \EndFor
    \EndFor
\EndFor
\EndFunction
\end{algorithmic}
\end{multicols}
%\end{\algsize}
\end{algorithm*}
   #+end_export


   Turning to the second example, the one of linear Gaussian CPDs we
   have for the local CPD

   #+begin_export latex
   \begin{align} \label{eq:like-gaussian-cpd}
   P(X|\theta) = &\prod_m \prod_{y_i \in Val(Y_i), pa_i \in Val(Pa_i)} \prod_{h[m] \in Val(\mathscr{H}[m])} \frac{1}{\sqrt{2\pi\sigma^2}} exp[-\frac{1}{2\sigma^2} (Q(h[m]) * y[m]  \\
               & - (\beta_0 + \beta_1 * pa_1[m] + ... + \beta_K * pa_K[m]))^2]  \nonumber
   \end{align}
   #+end_export

   such that once more we have an exponential family, which likelihood
   we aim to optimize.

   In order to perform such a task we refer to the M-projection
   theory. As proved by cite:koller2009probabilistic, the M-projection
   of an arbitrary distribution on the exponential family is given by
   parameterization where the expected sufficient statistics of the
   two distributions match.

   Moreover, given the fact that it is possible to prove that the MLE
   of an exponential family is nothing else than the M-projection of
   the empirical distribution on the exponential distribution of
   interest, it follows immediately that we can find the MLE
   parameterization by finding the M-projection through
   moment-matching.

   In the specific to solve such MLE problem we need to find the
   parameterization such that the empirical average of the sufficient
   statistics corresponds to the one of the expected sufficient
   statistics given the exponential family parameterization.

   Given the above results from information theory it is generally
   possible to compute the MLE of exponential families in the presence
   of missing data by firstly computing a map

   $$ess(\theta) = E_{P_\theta}(E_Q(\tau(Y)))$$

   Then, if possible, inverting such map

   $$\theta = ess^{-1}$$

   and finally inserting the empirical moments of the expected
   sufficient statistics.

   Note that due to the synthetically completed dataset you work with
   the expected - expected sufficient statistics. Where the double
   expectation has to account on the one hand the expectation of the
   synthetically completed evidence and, on the other hand, the moment
   matching expectation given the exponential family parameterization
   from the M-projection theory.

   Doing the above exercise for a simple linear Gaussian CPD with a
   single parent we would get the following picture

   #+begin_export latex
   \begin{align*}
   ess (\theta) &= ess\begin{pmatrix}
                   \beta_0\\
		   \beta_1
		   \end{pmatrix} \\
		   &= \begin{pmatrix}
		   E_{P_\theta}(E_Q(Y)) = \beta_0 + \beta_1 E_{P_\theta}(Pa_1) \\
		   E_{P_\theta}(E_Q(Y * Pa_1)) = \beta_0 E_{P_\theta}(Pa_1) + \beta_1 E_{P_\theta}(Pa_1^2)
		   \end{pmatrix}
  \end{align*}
   #+end_export


   Such that inverting such a map and inserting the empirical moments
   we get
   
   #+begin_export latex
   \begin{align}
   \hat{\theta} &= \begin{pmatrix}
                   \hat{\beta_0}\\
		   \hat{\beta_1}
             \end{pmatrix} 
          = \begin{pmatrix}
		   E_D(E_Q(Y)) - \frac{E_D(E_Q(Y*Pa_1))- E_D(E_Q(Y))E_D(Pa_1)}{E_D(Pa_1^2) - E_D(Pa_1)^2} * E_D(Pa_1)\\
		   \frac{E_D(E_Q(Y*Pa_1))- E_D(E_Q(Y))E_D(Pa_1)}{E_D(Pa_1^2) - E_D(Pa_1)^2}
             \end{pmatrix}
  \end{align}
   #+end_export

   where the empirical moments are given by $E_D(E_Q(Y)) = \frac{1}{M}
   \sum_m \sum_{h[m] \in Val(\mathscr{H}[m])} Q(h[m]) y[m]$ and
   similar.    

   It is now clear that such an approach can be used in the general
   case of exponential families. You can for instance easily get to
   the MLE result of the multinomial case achieved via Lagrange method
   through the moment matching idea presented above.

   In general the methodical frame for exponential families CPDs is
   the following; you substitute the inference step in line 27 of
   Algorithm [[ref:alg:EM-Likelihood-Missing-Data]] with an inference step
   calculating the expected sufficient statistics /of interest/ given
   the exponential family distribution of choice. You then insert in
   the M-step of line 6-9, the M-projection parameterization obtained
   by the moment-matching of expected sufficient statistics as
   discussed above. Finally you iterate until convergence.

** Bayesian Parameter Learning
   :PROPERTIES:
   :CUSTOM_ID: bayes-parameter-learning
   :END:
   
   A natural question that arises is whether it is possible to
   generalize the extended algorithm proposed by cite:Mrad_2015 to the
   case of Bayesian Parameter Learning.

   Recall that in Bayesian statistics rather than treating the
   parameters of interest as fixed but unknown variables you treat
   them as random variables themselves.

   You would then specify a prior, i.e. a probability distribution, for
   the data governing process of the parameters. This can be either a
   non-informative prior or a prior based on your domain knowledge
   expertise.

   Such prior distribution would then be updated upon the arrival of
   new observations according to the well known Bayes Rule. The result
   is an updated posterior distribution from which you can compute your
   statistics of interest.


   #+begin_export latex
   \begin{equation} \label{eq:bayes_formula}
   P (\theta | \mathscr{D}) = \frac{P (\mathscr{D} | \theta) * P(\theta)}{P (\mathscr{D})} 
   \end{equation}
   #+end_export

   It is straightforward to see that that the posterior is proportional
   to a likelihood term $P (\mathscr{D} | \theta)$ multiplied by the
   prior distribution.

   It is clear then, that depending on how you want to leverage the
   information of your posterior for finding a point estimate for your
   network parameterization, you would require a different
   mathematical exercise. I.e. in case you want to use as your point
   estimate of choice the expected value you would need an integration
   exercise and similar reasonings can be done for the other metrics.

   As argued in the introductory chapter, another way you can set your
   parameters is by choosing the most likely point estimate. This is
   point estimate maximizing your posteriori likelihood, i.e. it is
   defined as:

   #+begin_export latex
   \begin{align} \label{eq:bayes_map}
   \tilde{\theta} =& \operatorname*{argmax}_{\theta} \frac{P (\mathscr{D} | \theta) * P(\theta)}{P (\mathscr{D})} \nonumber\\
   \tilde{\theta} =& \operatorname*{argmax}_{\theta} P (\mathscr{D} | \theta) * P(\theta)\\ 
   \tilde{\theta} =& \operatorname*{argmax}_{\theta} log (P (\mathscr{D} | \theta)) + log (P(\theta)) \nonumber
   \end{align}
   \begin{align} \label{eq:bayes_map2}
   score_{MAP} (\theta : \mathscr{D}) =& \ log (P (\mathscr{D} | \theta)) + log (P(\theta)) \nonumber\\
   \nonumber\\
   \tilde{\theta} =& \operatorname*{argmax}_{\theta} score_{MAP}(\theta : \mathscr{D}) 
   \end{align}
   #+end_export

   Where the last equation in [[ref:eq:bayes_map]] follows immediately
   from the properties of the logarithm function. And the second
   equation in ref:eq:bayes_map from the fact that the normalizing
   constant does not depend on the parameter of interest.

   Given the above it is possible to understand that the conclusions
   from the previous chapter about the EM algorithm apply. The first
   term of $score_{MAP}$ is exactly the likelihood term of the previous
   section. The only difference will be in the prior distribution term.

   We will show next that it is possible to adjust the M-step of the EM
   algorithm in order to have a properly working EM algorithm
   maximizing the score map of [[ref:eq:bayes_map2]]. This will be the main
   exercise of the next section.

*** Bayesian Parameter Learning - EM Generalization

    Maximum a posteriori Bayesian Parameter Learning is a
    straightforward generalization of the discussion of [[ref:math_em]].

    In fact noting that the score of the MAP estimator is defined as

    #+begin_export latex
    \begin{equation} 
    score_{MAP} (\theta : \mathscr{D}) =& \ log (P (\mathscr{D} | \theta)) + log (P(\theta)) 
    \end{equation}
    #+end_export

    it is possible to see that the previous results apply.

    In order to see that define the following adjusted energy
    functional:
   
    #+begin_export latex
    \begin{equation} \label{eq:adj_energy_functional}
    \tilde{F}[\theta, Q] = E_Q[log (\tilde{P}(X))] + H_Q (X) + log (P(\theta)) 
    \end{equation}
    #+end_export

    Such that:

    #+begin_export latex
    \begin{align} \label{eq:adj_likelihood_energy_functional_relation}
    l (\theta: \mathscr{D}) + log (P(\theta)) =& \ \tilde{F}_D[\theta, Q] + D (Q (\mathscr{H}) || P (\mathscr{H}| \theta, \mathscr{D})) 
    \end{align}
    #+end_export  

    It follows immediately that choosing $Q$ as $P
    (\mathscr{H}|\mathscr{D}, \theta)$ and
    maximizing the adjusted energy functional we are in fact maximizing
    the score-map such that the results of the previous section
    apply. 

    The only question remaining is on how to optimize the adjusted
    energy functional via coordinate ascent optimization.

    Here it is straightforward to see that the adjusted metric does not
    affect E-step (we still choose Q in the very same way) but the
    M-step needs to be reformulated taking the effect of the prior into
    account.

    In order to see this consider our discussion in the previous
    chapter. The way you choose the Q distribution is unaffected and
    we will need to perform the same exercise in order to get the
    $\operatorname*{argmax}_{Q} \tilde{F}_D[\theta, Q]$.

    However, what is affected is the optimization along the other
    coordinate. That is the computation of
    $\operatorname*{argmax}_{\theta} \tilde{F}_D[\theta, Q]$ keeping Q
    fixed. In this case the terms depending on \theta is not limited to
    the expected likelihood $E_Q[l (\theta: \mathscr{D}, \mathscr{H})]$
    as was the case before but it is rather important to also consider
    the prior distribution $P(\theta)$.

*** Bayesian Parameter Learning - A CPT example
    :properties:
    :custom_id: cpt:cpt_bayes_learning
    :end:

    An example for the extension of the EM algorithm to compute the
    maximum a posteriori parameter in the case of missing evidence is
    treated in this section.

    The theory proceeds with the most classic network structure. The
    one of table conditional probability distributions where the
    realizations are distributed according to a multinomial
    distribution given the \theta_{X_i | Pa_{X_i}} local parameters and
    where possible realizations are binary, $Val(X_i) = \{0,1 \}$.

    Specifying a Dirichlet distribution as the prior of such parameters
    we can compute the maximum a posteriori estimator.

    As from the reasoning of the previous chapter we know that the EM
    algorithm properties of convergence and correctness apply and that
    the algorithm will iteratively converge to a local maximum.

    While as mentioned the E-step will be unaffected by the
    introduction of the prior, we need to adapt the M-step to account
    for the influence of the latter.

    Consider in this sense the unnormalized probability for the
    Dirichlet-Multinomial posterior distribution:

    #+begin_export latex
    \begin{align} \label{eq:dirichlet-multinomial-score}
    P(\theta | X) = \frac{\Gamma(\sum_i x_i + 1)}{\prod_i \Gamma(x_i + 1)} \prod_i^K \theta_{x_i | Pa_i}^{x_i}  * \frac{1}{B(\alpha)} \prod_{i=1}^K \theta_{x_i | Pa_i}^{\alpha_i - 1}
    \end{align}
    #+end_export

    And consider the adjusted energy functional
    [[ref:eq:adj_energy_functional]]. We can derive the new likelihood
    expression in the case of missing evidence by defining a new
    random variable $Y$ expressing synthetically completed data observations $<H,
    D>$:
   
    #+begin_export latex
    \begin{align} \label{eq:dirichlet-multinomial-likelihood}
    \tilde{F}[\theta, Q] =& \ E_Q[P_\theta(Y)] + H_Q (Y) + log(P_{hyperparameters}(\theta))
    \end{align}
    #+end_export

    Such that taking the argument maximizing the likelihood of the
    adjusted energy functional $\operatorname*{argmax}_{\theta}
    \tilde{F}[\theta, Q]$ we are left with the following with y[m]
    representing synthetically created complete observation <h[m],
    d[m]>:

    #+begin_export latex
    \begin{align} \label{eq:first-order-condition}
    \tilde{\theta} =& \operatorname*{argmax}_{\theta} \sum_m E_Q[log(\frac{\Gamma(\sum_i y[m]_i + 1)}{\prod_i \Gamma(y[m]_i + 1)} \prod_i^K \theta_{y_i | Pa{y_i}}^{y[m]_i} * \frac{1}{B(\alpha)} \prod_{i=1}^K \theta_{y_i | Pa{y_i}}^{\alpha_i - 1})] + H_Q (y[m]) \\
    \nonumber\\   
    \tilde{\theta} =& \operatorname*{argmax}_{\theta} \sum_m E_Q[log(\prod_i^K \theta_{y_i | Pa{y_i}}^{y[m]_i} * \theta_{y_i | Pa{y_i}}^{\alpha_i - 1})]\\
    \nonumber\\   
    \tilde{\theta} =& \operatorname*{argmax}_{\theta} \sum_m E_Q[log(\prod_i^K \theta_{y_i | Pa{y_i}}^{y[m]_i + \alpha_i - 1})] 
    \end{align}
    #+end_export

    It follows given that by the linearity of the expectation and that
    $y[m]_i = \{0,1\}$, we can re-express the above as:
   
    #+begin_export latex
    \begin{align} \label{eq:solution1}
    \tilde{\theta} =& \operatorname*{argmax}_{\theta} \sum_i^K (\sum_m^M E_Q[M[y_i, Pa_{y_i}]] + \alpha_i - 1) * log(\theta_{y_i | Pa{y_i}})] 
    \end{align}
    #+end_export

    where it holds

    #+begin_export latex
    \begin{align} \label{eq:expected_sufficient}
    \bar{M}[y_i, Pa_{y_i}]  =& \sum_m^M E_Q[M[y_i, Pa_{y_i}]]\\
    \bar{M}[y_i, Pa_{y_i}]  =& \sum_m^M \sum_{h[m] \in Val(\mathscr{H}[m])} Q(h[m]) \mathbbm{1}_{\{Y[m]_i = y[m]_i\}}\\
    \bar{M}[y_i, Pa_{y_i}]  =& \sum_m^M P(y_i | d[m], \theta)
    \end{align}
    #+end_export   

    So that ultimately:
   
    #+begin_export latex
    \begin{align} \label{eq:solution2}
    \tilde{\theta} =& \operatorname*{argmax}_{\theta} \sum_i^K (\bar{M}[y_i, Pa_{y_i}] + \alpha_i - 1) * log(\theta_{y_i | Pa{y_i}})] 
    \end{align}
    #+end_export      

    Given the additional restriction that $\sum_i \theta_{y_i |
    Pa{y_i}} = 1$, we can obtain the necessary condition for finding
    the optimum by using the Lagrange method

    #+begin_export latex
    \begin{align} \label{eq:first-order1}
    \frac{\partial}{\partial \theta_{y_i | Pa{y_i}}} \sum_i^K (\bar{M}[y_i, Pa_{y_i}] + \alpha_i - 1) * log(\tilde{\theta}_{y_i | Pa{y_i}})] - \lambda (\sum_i \tilde{\theta}_{y_i | Pa{y_i}} - 1) \mathrel{\stackon[5pt]{$=$}{$\scriptstyle!$}} 0
    \end{align}
    \begin{align} \label{eq:first-order2}
    \lambda = \frac{\bar{M}[y_i, Pa_{y_i}] + \alpha_i - 1}{\tilde{\theta}_{y_i | Pa{y_i}}}
    \end{align}
    #+end_export

    And inserting this in the first order condition and solving for
    $\tilde{\theta}_{y_i | Pa{y_i}}$

    #+begin_export latex
    \begin{align} \label{eq:solution}
    \tilde{\theta}_{y_i | Pa{y_i}} =& \frac{\bar{M}[y_i, Pa_{y_i}] + \alpha_i - 1}{\sum_j \bar{M}[y_j, Pa_{y_j}] + \alpha_j - 1}
    \end{align}
    #+end_export

    This will be the way you update the parameters in the M-step.

    It is straightforward to see from the above that it is possible to
    perform the same exercise in similar settings possibly leveraging
    the M-projection theory in the case of exponential family posterior
    distributions. 

    We conclude by noting that as long as the prior distribution
    $P(\theta)$ is well behaved in the sense that the resulting
    posterior (i) is concave (ii) is differentiable (iii) is smooth
    such that it is possible to exchange differentiation and
    integration; then the MAP estimator will exists, the correctness
    and convergence properties of EM apply to the score of the maximum
    a posteriori point estimate and we will choose a local maximum
    point estimator for the likelihood of the observed data.

*** Bayesian Parameter Learning - An Exponential Family Generalization

    This section generalizes the exercise of the above section for
    general exponential family distributions. As discussed in
    cite:barndorff1978hyperbolic, cite:geiger1998asymptotic,
    cite:lauritzen1996graphical and as well known from standard
    statistical theory such distributions are particularly well suited
    for statistical analysis due to their properties. 
    
    Albeit the only restriction for the choice of the prior
    distribution are the one mentioned at the end of the previous
    section a particularly sensible selection for the prior
    distribution is the one of using conjugate priors as defined by
    cite:schlaifer1961applied. This because, when using conjugate
    priors the data is incorporated into the posterior distribution
    only through the sufficient statistics such that there will exist
    relatively simple formulas for updating the prior into the
    posterior cite:fink1997compendium.

    Moreover, through such a property it will be easy to compute the
    MLE according to the sufficient statistics in the /complete data/
    case, or according to the expected sufficient statistics in the
    case of /missing data/ evidence. Finally, the fact that conjugate
    priors of exponential family distributions will often be well
    known exponential family distributions will further help in the
    parameter estimation given that the maximum for such posterior
    distributions are well documented in many statistical textbooks.

    You can note in fact that the Dirichlet prior chosen in the previous
    section is nothing else then the conjugate prior to the
    multinomial distribution. Note however that the resulting
    posterior is not an exponential distribution such that you cannot
    apply the M-projection theory to get the result above.

    Turning to the linear Gaussian parametric model presented in this
    manuscript it is possible to see that the conditional distribution
    of local nodes in the network arises by a multivariate normal
    distribution of the parents, see for instance
    cite:koller2009probabilistic.

    It follows therefore that one way for performing Bayesian
    parameter learning in linear Gaussian Bayesian networks is by
    specifying a normal-inverse Wishart prior distribution on the
    multivariate mean and co-variance matrix of the local nodes
    parents.

    After obtaining the new posterior hyperparameters depending on the
    prior hyperparameters and the sufficient (expected) sufficient
    statistics in the case of complete (missing) data, it is possible
    to obtain the maximum by getting the mode of the resulting
    multivariate t-distribution parameterized according to the
    hyperparameters and expected sufficient statistics.

    Similar reasonings are possible by specifying accordingly the CPDs
    and prior distributions, such that a rich modeling set is
    available and easily implementable in statistical software.
    
* On Likelihood Evidence

    Recall that as defined in cite:Mrad_2015 in likelihood evidence an
    observation is uncertain due to unreliable source of information.

    Here evidence in a finding is expressed as a vector containing the
    relative likelihood of a random variable realization. Consider for
    instance a random variable *X* then its likelihood evidence is
    defined as:

    #+begin_export latex
    \begin{align} \label{eq:likelihood-evidence}
     L(X) = (L(X = x_1): ... : L(X = x_k))
    \end{align}
    #+end_export

    Or when normalized you can express the likelihood-evidence as 

    #+begin_export latex
    \begin{align} \label{eq:normalized-likelihood-evidence}
     L(X) = (P(obs | x_1): ... : P(obs | x_k))
    \end{align}
    #+end_export    

    Note that here the relative likelihoods do not have to sum to
    one. Thus they cannot be not be interpreted as probabilities.

    Moreover, the key take-away that distinguish likelihood evidence
    from probabilistic evidence is, as mentioned, the fact that a
    likelihood evidence vector as in [[ref:eq:likelihood-evidence]] is
    specified without a prior. This means that the prior encoding the
    probabilistic structure of the network for the local random
    variable realization is not taken into account. I.e. the
    information resulting from $P(X_i|Pa(X_i))$ is not considered when
    expressing such an evidence.

    This means, that when updating the belief on the realization of
    the random variable *X_i*, i.e. at inference time, the likelihood
    evidence provided by the unreliable source of information must be
    combined with the prior probability resulting from the
    probabilistic structure implied by the network.

    We will turn next to the task of doing such inference and the task
    of parameter learning under likelihood evidence describing the
    approach proposed in cite:Wasserkrug_all.


** Adjusted EM - Likelihood Evidence

   One of the most widespread ways to deal with likelihood evidence
   was introduced by cite:pearl2014probabilistic. The idea is to
   remodel the network structure $G(\mathscr{V}, \mathscr{X})$ in order
   to represent the likelihood evidence as a hard-finding on a newly
   created /virtual-node/.

   Consider the Asia Network of Figure [[ref:fig:AsiaNet]], as in
   cite:Wasserkrug_all, cite:Mrad_2015. On the left hand side the core
   network is presented. Given hard findings or missing evidence we
   can estimate the parameters of the network via the standard
   EM-algorithm.

   Consider now the right hand side of Figure [[ref:fig:AsiaNet]]. Assume,
   as in cite:Wasserkrug_all that likelihood evidence is obtained for
   the Dysponea node via an NLP tool [NLP] analyzing historical
   medical records. Then as proposed by cite:pearl2014probabilistic we
   augment the network as on the right hand side of Figure
   [[ref:fig:AsiaNet]] by creating a child node of the Dysponea node. Such
   a child node will encode the likelihood evidence as hard finding by
   specifying the relation between Dysponea and Dysponea Observed of
   interest; i.e. it will encode the likelihood evidence via the CPD
   of $P(DysponeaObs | Dysponea)$.

   #+begin_export latex
\begin{figure}[!h]\vspace{2mm}
  \centering
  \caption[Asia Network]{Asia Network - Virtual Evidence Comparison}
  \label{fig:AsiaNet}
  \vspace{2mm}
  \begin{subfigure}[t]{0.4\linewidth} \label{subfig:missing}
	\begin{tikzpicture}[node distance={25mm}, main/.style = {draw, align=center}]
	%% Nodes
	\node[main] (1) {Asia Visit};
	\node[main][right of=1] (2) {Smoker?};

	\node[main][below of=1] (3) {Tubercolosis?};

	\node[main][right of=3] (4) {Lung Cancer?};
	\node[main][below right of=2] (5) {Bronchitis};

	\node[main][below right of=3] (6) {Tubercolosis\\Or Cancer?};          

	\node[main][below left of=6] (7) {Positive X-Ray?};

	\node[main][below right of=6] (8) {Dyspnoea?};     


	%% Edges
	\draw[->] (1) -- (3);
	\draw[->] (2) -- (4);
	\draw[->] (2) -- (5);
	\draw[->] (3) -- (6);     
	\draw[->] (4) -- (6);     
	\draw[->] (6) -- (7);               
	\draw[->] (5) -- (8);
	\draw[->] (6) -- (8);
	\end{tikzpicture}
        \vspace{5mm}
    \caption{Asia Network - Missing Evidence.\\}
  \end{subfigure} \hspace{15mm} 
  \begin{subfigure}[t]{0.4\linewidth} \label{subfig:virtual}
	\begin{tikzpicture}[node distance={25mm}, main/.style = {draw, align=center}]
	%% Nodes
	\node[main] (1) {Asia Visit};
	\node[main][right of=1] (2) {Smoker?};

	\node[main][below of=1] (3) {Tubercolosis?};

	\node[main][right of=3] (4) {Lung Cancer?};
	\node[main][below right of=2] (5) {Bronchitis};

	\node[main][below right of=3] (6) {Tubercolosis\\Or Cancer?};          

	\node[main][below left of=6] (7) {Positive X-Ray?};

	\node[main][below right of=6] (8) {Dyspnoea?};     
	\node[draw, distance={10mm}][below of=8] (9) {Dyspnoea \\ Obs};

	%% Edges
	\draw[->] (1) -- (3);
	\draw[->] (2) -- (4);
	\draw[->] (2) -- (5);
	\draw[->] (3) -- (6);     
	\draw[->] (4) -- (6);     
	\draw[->] (6) -- (7);               
	\draw[->] (5) -- (8);
	\draw[->] (6) -- (8);
	\draw[->] (8) -- (9);

	\end{tikzpicture}
        \vspace{5mm}
    \caption{Asia Network - Expanded as by Pearl's Virtual Evidence.}
  \end{subfigure}
  \vspace{0mm}
\end{figure}
   #+end_export

   Concretely assume as in cite:Wasserkrug_all that the NLP correctly
   characterizes Dysponea 70% of the times, when this does in fact
   occurs. Note that the NLP tool does not consider any prior
   information resulting from the probabilistic structure of our
   network. Then you might encode such likelihood evidence of the NLP
   as in Table [[ref:tb:virt-evidence]].

   #+begin_export latex
   \begin{table}[!h]

   \begin{center}
   \begin{tabular}{|l||*{2}{c|}}\hline
   \backslashbox{DysponeaObs}{Dysponea?}
   &\makebox[3em]{yes}&\makebox[3em]{no}\\\hline\hline
   True & 0.7 & 0.3\\\hline
   False & 0.3 & 0.7 \\\hline
   \end{tabular}
   \end{center}

   \caption[Virtual Evidence CPT]{DysponeaObs - Virtual Evidence Node CPT}
   \label{tb:virt-evidence}
   \end{table}
   #+end_export

   Given such a CPT, encoding the likelihood evidence, it is possible
   to set the DyspnoeaObs to true as a hard finding. In such a way you
   will work with a standard network that is just composed of missing
   and hard evidence. You can then update the cognitive state of your
   network by standard inference techniques, and compute the
   parameters of interest by a standard EM-algorithm.

   Given such explanation it follows that it is possible to rewrite
   the EM-step by adjusting the E-step such that it will perform its
   inference step on the virtual evidence augmented network that
   respects and incorporates the likelihood evidence information. This
   was the intuition and contribution of cite:Wasserkrug_all and such
   an algorithm, with the corresponding modification of the E-step, is
   presented in [[ref:alg:EM-Likelihood]].

   We continue the next section by modifying such algorithm such that
   it is possible to perform MAP estimation in Bayesian settings.

      
   #+begin_export latex
\algrenewcommand\algorithmicindent{1.5em}%

\begin{algorithm*}[h!]
\caption{EM-Likelihood: an EM algorithm for learning with likelihood evidence}
\label{alg:EM-Likelihood}
%\begin{\algsize}
\vspace{-10pt}
\begin{multicols}{2}
\begin{algorithmic}[1] 
\Require Bayesian network $\mathcal{B}=\langle \mathbf{X},\mathbf{D}, G, \mathbf{P} \rangle$, dataset $S$ 

\Procedure{EM}{$\mathcal{B}$, $S$}
\State Initialize $\mathcal{B}$'s parameters $\theta \leftarrow \theta^0$
\ForAll{$t=1, \ldots$ until convergence}
  \State $M-step \ as \ in \ Algorithm \ 1$
\EndFor
\EndProcedure
\\
\Function{Compute-ESS}{$\mathcal{B}=(G,\theta)$, $S$} 

\ForAll {$i\in1,\ldots,n$}
  \ForAll {$x_{i},u_{i}\in Val(X_{i},Pa_{X_{i}}^{\mathcal{B}})$}
   \State $\bar{M}[x_{i},u_{i}]\leftarrow 0$
  \EndFor
\EndFor

% \State (Go over all evidence nodes, creating an augmented network
% for each one, and collect all of the evidence for the nodes in $G$)
\ForAll{example $S_{j}\in S$}

    \State Let $O_j$ be the observations induced by $S_j$
    \State $(G',\theta') \leftarrow$ \textsc{Augment-BN}($\mathcal{B}=(G,\theta)$, $O_{j}$)
    %  (We'll denote $<G',\theta'>$ by $BN_{i}$ as it is the BN induced by example $i$)
    \ForAll{$o \in O_j$}
      \State Set the value of $o_V$ to $true$
    \EndFor
    \State Run inference on $(G',\theta')$ with evidence $d_{j}$
    \ForAll{i$ = 1,\ldots,n$}
      \ForAll{$x_{i},u_{i}\in Val(X_{i},Pa_{X_{i}}^{\mathcal{B}})$}
    
        \State $\bar{M}[x_{i},u_{i}] \mathrel{{+}{=}} P_{(G',\theta')}(x_{i},u_{i}|d_{j})$
    
      \EndFor
    \EndFor
\EndFor
\EndFunction
\\
\Function{Augment-BN}{$\mathcal{B}=(G,\theta)$, $O$} 
  \State Initialize $G'\leftarrow G$, $\theta'\leftarrow\theta$
  \ForAll{$o\in O$}

    \State $G'_{\mathbb{V}}\leftarrow G'_{\mathbb{V}}\cup o_{V}$, $G'_{\mathbb{E}}\leftarrow G'_{\mathbb{E}}\cup(V,o_{V})$      \Comment{Add a new observation node to the graph and connect it to the relevant node}
    \ForAll{$c_{i}\in Conf$}   \Comment{$Conf$ actual likelihood values provided for a node}
      \State $\theta'\leftarrow\theta'\cup\theta_{O_{V}=true|v_{i}}=c_{i}$ \Comment{Set the relevant CPT entry to be $Pr(obs|V=v_{i})$}
    \EndFor
  \EndFor
\State \textbf{return} $(G',\theta')$
\end{algorithmic}
\end{multicols}
%\end{\algsize}
\end{algorithm*}
   #+end_export

   \newpage
   
** Bayesian Learning MAP - Adjusted EM for Likelihood Evidence 

   The idea of this section is to extend [[ref:alg:EM-Likelihood]] in
   order to obtain the MAP estimator in a Bayesian Learning setting
   with Likelihood Evidence.

   We discussed in the previous section how likelihood evidence
   requires augmenting the core network by virtual evidence nodes as
   in cite:pearl2014probabilistic and consequently perform the
   inference step on such augmented networks.

   Such procedure was outlined by the modification of the E-step in
   comparison to the standard EM algorithm with missing evidence.

   Moreover, we discussed in section [[ref:bayes-parameter-learning]], how
   it is possible to adjust the M-step of the EM-algorithm to perform
   the task of MAP estimation. Both correctness and convergence
   properties will apply such that we will converge to a local maximum
   for our posterior distribution.

   Combining the two steps it is immediate to see that it is possible
   to perform Bayesian Parameter Learning under likelihood evidence
   by replacing line 4 of [[ref:alg:EM-Likelihood]] with 

   #+begin_export latex
\begin{algorithm*}[h!]
\caption{Replace M-step for Bayesian Parameter Learning}
\label{alg:Bayes-EM-Likelihood}
%\begin{\algsize}
\vspace{-10pt}
\begin{multicols}{2}
\begin{algorithmic}[1] 
\Require Bayesian network $\mathcal{B}=\langle \mathbf{X},\mathbf{D}, G, \mathbf{P} \rangle$, dataset $S$ 

\Function{M-Step}{$\mathcal{B}$, $S$}
   \State $\theta_{x_{i}|u_{i}}^{t+1}=\frac{\bar{M}_{\theta^{t}}[x_{i},u_{i}] + \alpha_i - 1}{\sum_j \bar{M}_{\theta^{t}}[x_{j},u_{j}] + \alpha_j - 1}$\\
   
   \textbf{return} $(\theta^{t+1})$

\end{algorithmic}
\end{multicols}
%\end{\algsize}
\end{algorithm*}
   #+end_export

   Given such a computation it is possible to get to a local maximum
   for the MAP estimator.


* On Probabilistic Evidence

  In the previous chapter we showed how it is possible to rephrase a
  likelihood evidence as an hard evidence by means of augmenting the
  network via /virtual evidence/.

  We could then perform the inference step and propagate the
  information by means of Bayes Rule updating the probabilistic
  structure of the network.

  In this section we extend the theory presented that far by
  introducing some techniques in order to deal with parameter learning
  for the case of /non-fixed probabilistic evidence/.
  
  By contrast, with probabilistic evidence such an approach is not
  viable. This because, as argued by cite:PENG_2010, propagating a
  probabilistic finding on $X_i \in \textbf{X}$ requires a revision of
  the probability distribution of the network $P_\theta(\textbf{X})$
  on $X_i$ by a local probability distribution defined by the
  probabilistic evidence statement $R(X_i)$. And given that $R(X_i)$,
  although acting as a condition for the update, is not itself an
  event, Bayes Rule and standard inference based on message passing
  algorithms fail. Hence, as mentioned in cite:Mrad_2015, a
  probabilistic finding $R(X_i)$ requires a reconsideration of the
  entire joint probability distribution $P_\theta(\textbf{X})$ because
  it replaces the existing /prior/ on the variable $X_i$.

  In simple words, in the presence of probabilistic evidence it is not
  possible to propagate evidence by standard message passing
  algorithms. The solution proposed by cite:jeffrey1990logic, is then
  to replace the initial probabilistic structure of the network
  $P_\theta(\textbf{X})$ by a new probabilistic structure
  $Q_\theta(\textbf{X})$ that reflects the beliefs in the variables of
  the model /after accepting the probabilistic evidence/.

  In the specifics, as well outlined by cite:Mrad_2015, according to
  what is usually referred as Jeffrey's /probability kinematics/, $Q$
  must satisfy the following requirements:

  1. the posterior probability distribution considering the network
     structure on the observed variable $Q(X_i)$ is unchanged: $Q(X_i)
     = R(X_i)$. This is in fact the functional requirement of the
     probabilistic evidence.

  2. the conditional probability distribution of other variables given
     $X_i$ remains invariant under the observation: $Q(\textbf{X}
     \textbackslash X_i | X_i) = P (\textbf{X} \textbackslash X_i |
     X_i)$. This essentially means that even if P and Q disagree on
     $X_i$, they agree on the consequences of $X_i$ on other variables
     cite:Mrad_2015.

  With the above specification of a new probabilistic structure
  satisfying the functional requirements of probabilistic evidence it
  is possible to compute the probability of a given event by means of
  Jeffrey's rule:

  #+begin_export latex
  \begin{equation} \label{eq:Jeffreys_Update}
   Q(Z = z) = \sum_{x_i \in Val(X_i)} P(Z = z | X = x_i) R(X = x_i)
  \end{equation}
  #+end_export   

  Albeit being theoretically compelling, Jeffrey's formula above
  [[ref:eq:Jeffreys_Update]], cannot be directly applied in Bayesian
  Networks. In fact such a formula requires the specification and
  functional form of the full probabilistic structure of the network
  in any state of the network in order to compute $P(Z = z | X_i =
  x_i)$. I.e. in order to compute the new probabilistic structure
  Q_\theta you would need to perform an inference step over all
  possible sates combinations. A very computationally intensive task.

  The solution to this problem as suggested by cite:Chan_2005 and
  cite:PENG_2010 is to frame probabilistic evidence into likelihood
  evidence by computing the likelihood ratio as defined by:

  #+begin_export latex
  \begin{align} \label{eq:probabilistic-to-likelihood-evidence}
   L(X_i) = (\frac{R(x_{i_1})}{P(x_{i_1})}: ... : \frac{R(x_{i_k})}{P(x_{i_k})})
  \end{align}
  #+end_export

  It is then possible to prove that propagating such likelihood
  evidence by means of Pearl's method as described in the previous
  section, is equivalent to propagating and obtain the probabilistic
  structure by means of Jeffrey's method [[ref:eq:Jeffreys_Update]].

  It is in fact possible to prove, as in cite:PENG_2010, that with
  such an approach the posterior probability of $X_i$ after propagating
  $L(X_i)$ by Pearlâ€™s method, is equal to $R(X_i)$.

  Given such theory it is straightforward to understand that
  in the case of a single probabilistic evidence we can easily learn
  the parameters of the Bayesian Network via the following adjustment
  of the *AUGMENT-BN* function of [[ref:alg:EM-Likelihood]]

   #+begin_export latex
\algrenewcommand\algorithmicindent{1.5em}%

\begin{algorithm*}[h!]
\caption{EM-Likelihood: an EM algorithm for learning with likelihood evidence}
\label{alg:EM-Probabilistic-Evidence}
%\begin{\algsize}
\vspace{-10pt}
\begin{multicols}{2}
\begin{algorithmic}[1] 
\Require Bayesian network $\mathcal{B}=\langle \mathbf{X},\mathbf{D}, G, \mathbf{P} \rangle$, dataset $S$, Observations $O$

\Function{Augment-BN}{$\mathcal{B}=(G,\theta)$, $O$} 
  \State Initialize $G'\leftarrow G$, $\theta'\leftarrow\theta$, $Conf \leftarrow \emptyset$
  \ForAll{$r_{j_i}\in ProbEv(x_j)$}  \Comment{$ProbEv$ is the passed probabilistic evidence. r are the states for the Node.}
    \State {$Conf \leftarrow Conf \cup \frac{r_{j_i}}{\mathbf{P}_{x_{j_i}}}$}
  \EndFor
  \ForAll{$o\in O$}
    \State $G'_{\mathbb{V}}\leftarrow G'_{\mathbb{V}}\cup o_{V}$, $G'_{\mathbb{E}}\leftarrow G'_{\mathbb{E}}\cup(V,o_{V})$      \Comment{Add a new observation node to the graph and connect it to the relevant node}
    \ForAll{$c_{i}\in Conf$}   \Comment{$Conf$ computed likelihood for a probabilistic node}
      \State $\theta'\leftarrow\theta'\cup\theta_{O_{V}=true|v_{i}}=c_{i}$ \Comment{Set the relevant CPT entry to be $Pr(obs|V=v_{i})$}
    \EndFor
  \EndFor
\State \textbf{return} $(G',\theta')$
\end{algorithmic}
\end{multicols}
%\end{\algsize}
\end{algorithm*}
   #+end_export

   This concludes the section. It is important to mention, to this
   point that in the case of multiple probabilistic evidence on
   different nodes, the above approach does not apply.

   This because, as shown by example in cite:PENG_2010, the algorithm
   above is not commutative and does not guarantee the functional
   requirement $R(X_i) = Q(X_i)$ for at least one node $i$. This
   intuitively because it just guarantees the property for the last
   virtual node for which inference is propagated using Pearl virtual
   evidence method.

   This is for instance what happens in cite:PENG_2010 with two
   probabilistic evidence, $R(X_1), R(X_2)$ and the property that
   $Q(X_1) \neq R(X_1)$ or $Q(X_2) \neq R(X_2)$, depending on the
   order of propagation.

   In order to solve such an issue, and guarantee the functional
   requirement of probabilistic evidence, the /Iterative Proportional
   Fitting Procedure (IPFP) algorithm/ was proposed by
   cite:Valtorta_2002.

   The algorithm is based on the following theorem as reported in
   cite:PENG_2010:

   #+begin_export latex
   \begin{theorem}\label{thm:two-I-projection}
   Let $Q(X_i)$ be the distribution resulted from updating $P(\textbf{X})$ by $R(X_i)$,
   $X_i \subset \textbf{X}$ using Jeffreyâ€™s rule described above. Then $Q(X_i)$ is the I-projection of $P(\textbf{X})$ on
   $\textbf{P}_{R(X_i)}$, where $\textbf{P}_{R(X_i)}$ is the set of distributions whose marginal over $X_i$ equal $R(X_i)$.
   \end{theorem}
   #+end_export
   
   The idea of the IPFP algorithm is then the one of allowing multiple
   probabilistic evidence by leveraging the theorem above.

   As well outlined in cite:PENG_2010, the idea is in fact to modify
   $P(\textbf{X})$ incorporating the multiple constraints arising from
   the multiple probabilistic evidence conditions passed by the
   user. Consider $j = 1, ..., J$ restrictions, then you can perform
   such an exercise by iteratively projecting the distribution
   resulted from the previous iteration $\textbf{P}_{R(X_j)}$ on the
   next set of constraints $R(X_{j+1})$.

   Formally the IPFP would look as follows:

   #+begin_export latex
\algrenewcommand\algorithmicindent{1.5em}%

\begin{algorithm*}[h!]
\caption{IPFP Algorithm}
\label{alg:IPFP-algorithm}
%\begin{\algsize}
\vspace{-10pt}
\begin{multicols}{2}
\begin{algorithmic}[1] 
\Require Probabilistic Evidence $R(X_j, ..., X_J)$, intial distribution $P(\textbf{X})$, necessary condition $R(X_j) << Q_{k-1}(X_j) \ \forall \ k$

\Function{IPFP-Distribution}{$Q_k(\textbf{X})$} 
  \State Initialize $Q_0(\textbf{X}) \leftarrow P(\textbf{X})$
  \For{$k = 1, ..., m = 1 + (k âˆ’ 1) \ mod \ J$}
    \State {

$Q_k(x) = \left\{
\begin{array}{ll}
Q_{k-1}(x) * \frac{R(x_j)}{Q_{k-1}(x_j)}
& Q_{k-1}(x_j) \geq 0 \\
0 & \, else \\
\end{array}
\right. $} \Comment{note that $j$ represent the constraint used at each iteration }
  \EndFor
\end{algorithmic}
\end{multicols}
%\end{\algsize}
\end{algorithm*}
   #+end_export

   As proved by several authors such method converge. Moreover, given
   the new probabilistic structure $Q_k(\textbf{X})$ that reflects the
   beliefs in the variables of the model /after accepting the
   probabilistic evidence/, we can learn the parameters of the network
   using the standard EM-algorithm.

   It holds in fact that given complete or missing evidence we can
   leverage the theory developed in the previous chapters to learn the
   parameters of a network displaying multiple probabilistic evidence
   by leveraging the updated network probabilistic structure
   $Q_k(\textbf{X})$ at the inference step in the E-step.

   This is summarized in algorithm [[ref:alg:EM-Probabilistic]]:

   #+begin_export latex
\algrenewcommand\algorithmicindent{1.5em}%

\begin{algorithm*}[h!]
\caption{EM-Proabilistic: an EM algorithm for learning with probabilistic evidence}
\label{alg:EM-Probabilistic}
%\begin{\algsize}
\vspace{-10pt}
\begin{multicols}{2}
\begin{algorithmic}[1] 
\Require Bayesian network $\mathcal{B}=\langle \mathbf{X},\mathbf{D}, G, \mathbf{P} \rangle$, dataset $S$ 

\Procedure{EM}{$\mathcal{B}$, $S$}
\State Initialize $\mathcal{B}$'s parameters $\theta \leftarrow \theta^0$
\ForAll{$t=1, \ldots$ until convergence}
  \State $M-step \ as \ in \ Algorithm \ 1$
\EndFor
\\
\Function{Compute-ESS}{$\mathcal{B}=(G, \theta)$}

\State {Run IPFP given current parameterization}\Comment {Note - you have to perform such  iteration at each iteration}
\State {$Q_k \leftarrow$ Return convergence distribution of algorithm IPFP above} 

\ForAll {$i\in1,\ldots,n$}
  \ForAll {$x_{i},u_{i}\in Val(X_{i},Pa_{X_{i}}^{\mathcal{B}})$}
   \State $\bar{M}[x_{i},u_{i}]\leftarrow 0$
  \EndFor
\EndFor

\ForAll{example $S_{j}\in S$}

    \State {Run inference on $(G, Q_k, \theta)$ with evidence $d_{j}$}

    \ForAll{i$ = 1,\ldots,n$}
      \ForAll{$x_{i},u_{i}\in Val(X_{i},Pa_{X_{i}}^{\mathcal{B}})$}

        \State $\bar{M}[x_{i},u_{i}] \mathrel{{+}{=}} Q_{(G,\theta)}(x_{i},u_{i}|d_{j})$ \Comment {Note that inference is based on the adjusted distriution Q obtained above}
      \EndFor
    \EndFor
\EndFor
\EndProcedure

\end{algorithmic}
\end{multicols}
%\end{\algsize}
\end{algorithm*}
   #+end_export
   
   This concludes the theory for Parameter Learning under
   probabilistic evidence. We note two problems, the first being that
   the algorithm needs to perform IPFP at each E-step given the new
   parameterization. You run in fact a chicken-egg problem that makes
   the solution to such problem very expensive. IPFP requires the
   parameterization of the node to be known in order to perform
   inference and get to the updated probabilistic structure of the
   network after consider probabilistic evidence. But we desire to
   learn such a parameterization such that we need to repeat the above
   until convergence.

   Moreover, we note that IPFP requires to run inference and compute
   the new probabilistic structure for all possible realizations in
   the network $x \in Val(\textbf{X})$. Moreover to perform such task
   the full joint probability distribution P(\textbf{X}) would be
   necessary. It is obvious that for such big Bayesian Networks such a
   task full-joint would be infeasible to compute. In this sense,
   further refinements were propose such a /big clique/ refinement as 
   discussed in cite:PENG_2010, which you could then plug-in
   [[ref:alg:IPFP-algorithm]].
   
   
* On Numerical EM

  As argued in the previous section when working with conjugate prior
  we might easily get to closed form solutions for the maximum of the
  posterior.

  However, as was previously discussed it might be limiting to
  restrict the prior specification to conjugate priors of exponential
  distributions.

  To tackle this issue and address the possibility of using a richer
  class of likelihood-priors instantiations we propose in this
  section some arguments for iteratively computing the maximum of an
  arbitrary well behaved distribution as discussed in section
  [[ref:cpt:cpt_bayes_learning]].

  One classical tool to perform the task is the one of leveraging
  stochastic simulation methods to sample from the posterior
  distribution of interest and leverage asymptotic theory to get to
  the statistics of interest.
       
  Another option, which we will focus next, is to apply a numerical
  solution to the M-step of the EM algorithm leveraging the theory
  presented in cite:ruud1989comparison.

  Hence, in this section, we will aim to generalize the theory
  presented that far such that it is possible to implement general
  statistical software without having to limit the end-user to very
  specific pre-defined cases, where the algorithm running in the
  background has necessarily to know the closed-form analytical
  solution of the M-step.

  Note that this will come at costs. We will need in fact to compute
  the Hessian of our expected log-likelihood which is one of the most
  computationally intensive tasks. This especially in highly
  dimensional problems. One of the major benefits in using the EM over
  gradient based methods would be lost in this sense.

** Numerical EM for MLE estimator

   In order to understand how to compute M-step according to an
   iterative method, think at the following.

   Consider that in the E-step you set $Q = P (\mathscr{H}| \mathscr{D}, \theta_0)$, such
   that you can reformulate
   [[ref:eq:likelihood_energy_functional_relation]] as follows

   #+begin_export latex
   \begin{align} \label{eq:likelihood_energy_iterative}
   l (\theta: \mathscr{D}) =& \ H_Q (\mathscr {H}) + \sum_h P(h | \mathscr{D}, \theta_0) * l (\theta: \mathscr{D}, \mathscr{H}) \\
   \nonumber\\
   Q(\theta, \theta_0 : \mathscr{D}) \eqdef& \sum_h P(h | \mathscr{D}, \theta_0) * l (\theta: \mathscr{D}, \mathscr{H})\\
   \nonumber\\  
   H(\theta_0, \theta: \mathscr{D}) \eqdef& \ Q(\theta, \theta_0 : \mathscr{D}) - l (\theta: \mathscr{D}) \\
                                    =& H_Q (\mathscr {H}) = \sum_h - P(h | \mathscr{D}, \theta_0) * P(\theta | h, \mathscr{D}) \nonumber
   \end{align}
   #+end_export  

   It follows
  
   #+begin_export latex
   \begin{align} 
   \frac{\partial}{\partial \theta} l (\theta: \mathscr{D}) =& \ l_1 (\theta: \mathscr{D}) = \frac{\partial}{\partial \theta} Q(\theta, \theta_0, \mathscr{D}) - \frac{\partial}{\partial \theta} H(\theta, \theta_0, \mathscr{D}) \nonumber \\
   =& Q_1(\theta, \theta_0 : \mathscr{D}) - H_1(\theta, \theta_0 : \mathscr{D})  \label{eq:m-condition-iterative1} \\
   \nonumber \\
   \frac{\partial^2}{\partial \theta \partial \theta'} l (\theta: \mathscr{D}) =& \frac{\partial^2}{\partial \theta \partial \theta'}  Q(\theta, \theta_0, \mathscr{D}) -  \frac{\partial^2}{\partial \theta \partial \theta'}  H(\theta, \theta_0, \mathscr{D}) \nonumber \\
     =& \ Q_{11}(\theta, \theta_0 : \mathscr{D}) - H_{11}(\theta, \theta_0 : \mathscr{D}) \label{eq:m-condition-iterative2}
   \end{align}
   #+end_export

   Moreover given the following condition

   #+begin_export latex
   \begin{align} 
    H_1(\theta_0, \theta_0 : \mathscr{D})  = 0 \tab \forall \theta_0 \label{eq:m-condition-entropy-iterative}
   \end{align}
   #+end_export

    we have for [[ref:eq:m-condition-iterative1]] that:

   #+begin_export latex
   \begin{align} 
    l_1(\theta_0: \mathscr{D})  = Q_1(\theta_0, \theta_0: \mathscr{D}) \tab \forall \theta_0 \label{eq:m-condition-entropy-iterative2} 
   \end{align}
   #+end_export

   Such that ultimately it holds using the classical derivation of the
   Newton-Raphson Method as in cite:storvik2007numerical:

  
   #+begin_export latex
   \begin{align} 
    \theta_{EM}  = \theta_{0} - Q_{11}^{-1} Q_1 + o(||\theta_{EM} - \theta_{0}||) \label{eq:em-iterative}
   \end{align}
   #+end_export  

   where both $Q_{11}, Q_{1}$ are evaluated at $\theta_0$.

   It follows immediately that for log-concave functions each iteration
   of [[ref:eq:em-iterative]] increases the likelihood. It is therefore
   possible to apply the above by inserting the numerical computed
   Hessian and gradient until convergence to a maximum.

   It is as well possible to set a predefined amount of iterations
   before switching to the next E-step in the EM-algorithm. Due to the
   increased computational cost of performing new inferences as well as
   computing new Hessian matrices such second option is not
   recommended; albeit being theoretically viable.

   As a final remark, note that methods to improve the computational
   speed of such numerical M-step have been proposed, such in
   cite:Louis_1982. As uphill steps cannot be guaranteed under all
   circumstances in such algorithm, we just refer to the literature
   the interested reader and do not consider this as a viable option
   for our solution. In that case the EM theory would collapse and
   there is no guarantee to reach a local maximum.

** Numerical EM for MAP estimator

   This section generalizes the arguments of the previous section to
   the case of MAP estimator in the case of Bayesian Parameter
   Learning.

   Using [[ref:eq:adj_energy_functional]] it follows immediately using the
   notation of the last section that:

   #+begin_export latex
   \begin{align} \label{eq:likelihood_energy_map_iterative}
   l (\theta: \mathscr{D}) + log(P(\theta)) =& \ H_Q (\mathscr {H}) + log(P(\theta)) + \sum_h P(h | \mathscr{D}, \theta_0) * l (\theta: \mathscr{D}, \mathscr{H})\\
   \nonumber\\
   Q(\theta, \theta_0 : \mathscr{D}) \eqdef& \ log(P(\theta)) + \sum_h P(h | \mathscr{D}, \theta_0) * l (\theta: \mathscr{D}, \mathscr{H})\\
   \nonumber\\  
   H(\theta_0, \theta: \mathscr{D}) \eqdef& \ Q(\theta, \theta_0 : \mathscr{D}) - l (\theta: \mathscr{D}) \\
                                    =& H_Q (\mathscr {H}) = \sum_h - P(h | \mathscr{D}, \theta_0) * P(\theta | h, \mathscr{D}) \nonumber
   \end{align}
   #+end_export  
   
   The idea is that as long as the likelihood and the prior are
   concave such that the sum of two concave functions will yield a $Q$
   function that is concave, we might apply the very same
   Newton-Raphson method to get iteratively to the maximum of the
   function.

   #+begin_export latex
   \begin{align} 
    \theta_{EM}  = \theta_{0} - Q_{11}^{-1} Q_1 + o(||\theta_{EM} - \theta_{0}||) \label{eq:em-iterative}
   \end{align}
   #+end_export

   where $Q_{11}, Q_1$ are defined as in the previous section and
   need now to account for the prior distribution influence.
   


** How to compute the numerical Hessian and Gradient

   This section concludes the chapter on Bayesian Parameter Learning
   by substituting the M-step of [[ref:alg:Bayes-EM-Likelihood]], by a
   numerical estimation of the maximum.

   Note, that as argued in the previous sections this has the benefit
   of allowing a general algorithm that is not bounded to the
   analytical derivation of the maximum in the M-step.

   #+begin_export latex
\begin{algorithm*}[h!]
\caption{Replace M-step for Bayesian Parameter Learning}
\label{alg:Numerical-M-Step}
%\begin{\algsize}
\vspace{-10pt}
\begin{multicols}{2}
\begin{algorithmic}[1] 
\Require Bayesian network $\mathcal{B}=\langle \mathbf{X},\mathbf{D}, G, \mathbf{P} \rangle$, dataset $S$, Current Parameterization $\theta_0$, Threshold $\epsilon$

\Function{M-Step}{$\mathcal{B}$, $S$}
   \State Numerically Compute $Q_1$
   \State Numerically Compute $Q_{11}$\\

   \ForAll{$t=0, \ldots$ until convergence}\\
      \State $\theta^{t+1}= \theta_{t} - Q_{11}^{-1} Q_1$\\
      \State convergence if $||\theta^{t+1} - \theta_{t}|| < \epsilon$
   \EndForAll
\end{algorithmic}
\end{multicols}
%\end{\algsize}
\end{algorithm*}
   #+end_export


*** TODO formulate the things in the paper below

    and incorporate them in the algo above.
   
**** TODO define how you compute such numerical derivative.

     note that you must have the guarantee of positive definite so that
     you move towards the maximum despite working with an approximate
     numerical term (with error).

     this goes together with next section notation. you can then use the
     method there in that paper.

     wow. so you thought well.

   
**** TODO note that the above Q terms are nothing else than the expected score and expected fisher information

     good paper in [[https://arxiv.org/pdf/1608.01734.pdf][this sense]].

     [[https://scholar.google.com/scholar?q=Method%20for%20Computation%20of%20the%20Fisher%20Information%20Matrix%20in%20the%20Expectation%2DMaximization%20Algorithm&btnG=Search&as_sdt=800000000001&as_sdtp=on][other link]].

     exactly what I was looking forward to.

     incorporate everything in the next step.
   

* Empirical Results

  

  \newpage
     


* TODOs
  
**** TODO check if particle formulation in energy functional ok as such
**** TODO note that network structure G(V,X) how you described. Correct later
**** TODO make more explicit the citation to koller and friedman in the chapter about the mathematics of the EM algo
**** TODO note that the EM you loose global decomposition such that at the end with the local maxima of CPDs achieved by EM you ultimately just get a local maximum.
**** TODO for last section - check at the following chapter.

     Concept: Representation Independence - in chapter 17 of the
     koller book. this is central as if that breaks apart the entire
     modeling and parameterization and models outlined here in the
     thesis fall apart.

     you should therefore be careful with it.
  
**** TODO check chapter 17 for making the point below
     
     say at the end that the entire theory does not hold for locally
     shared and globally shared network parameters. there you would
     have to adjust the entire thing

**** TODO have to insert as well as the sigma as parameter for the ess map in the exercise above
**** TODO REASON ON HOW JEFFERY's method can work with the EM algorithm

   \newpage

   bibliography:../literature/references.bib
   bibliographystyle:unsrt


* Footnotes

[fn:2] Think for instance to the necessary components of mathematical
   statistics, information theory, graph theory and optimization

[fn:1] Data from [[https://app.dimensions.ai/discover/publication][app.dimensions.ai]] 


